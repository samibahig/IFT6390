{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi_SVM:",
      "provenance": [],
      "authorship_tag": "ABX9TyMj4ZJ3vqlnZ4CKLZomYrcc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samibahig/IFT6390/blob/main/Multi_SVM_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlmfAcgaHI0C"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhWOzNJSHSOX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e1b88b82-7025-44d7-caa0-c05aaa4b7f69"
      },
      "source": [
        "class SVM:\n",
        "    def __init__(self, eta, C, niter, batch_size, verbose):\n",
        "        self.eta = eta\n",
        "        self.C = C\n",
        "        self.niter = niter\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def make_one_versus_all_labels(self, y, m):\n",
        "        \"\"\"\n",
        "        y : numpy array of shape (n,)\n",
        "        m : int (num_classes)\n",
        "        returns : numpy array of shape (n, m)\n",
        "        \"\"\"\n",
        "        dim1 = y.shape[0]\n",
        "        dim2 = m\n",
        "        xx=np.full((dim1,dim2),-1)\n",
        "   \n",
        "        for index, label in enumerate(y):\n",
        "           xx[index][label]=1        \n",
        "        return xx\n",
        "\n",
        "    \n",
        "\n",
        "    def compute_loss (self, x, y):\n",
        "        \"\"\"\n",
        "        x : numpy array of shape (minibatch size, num_features)\n",
        "        y : numpy array of shape (minibatch size, num_classes)\n",
        "        returns : float\n",
        "        \"\"\"\n",
        "        xw= np.matmul(x,self.w)\n",
        "        wxy= 2-xw*y                    #2-xw.y for each element of matrix\n",
        "\n",
        "        wxy = wxy.clip(min=0)         #make all negative elements zero\n",
        "\n",
        "        wxy_sq = wxy**2               #square all values\n",
        "\n",
        "        #apply sum here\n",
        "        sum_wxy_sq = np.sum(wxy_sq)\n",
        "        #print (\"sum_wxy_sq\", sum_wxy_sq)        \n",
        "\n",
        "        minibatch_size= x.shape[0]\n",
        "        term1 = sum_wxy_sq\n",
        "\n",
        "        term2 = np.sum(self.C*0.5*(self.w)**2)\n",
        "\n",
        "        loss = (term1)/(minibatch_size) + term2\n",
        "\n",
        "        #print (\"loss\", loss)\n",
        "        return loss\n",
        "\n",
        "   \n",
        "\n",
        "    def compute_gradient (self, x, y):\n",
        "        \"\"\"\n",
        "        x : numpy array of shape (minibatch size, num_features)\n",
        "        y : numpy array of shape (minibatch size, num_classes)\n",
        "        returns : numpy array of shape (num_features, num_classes)\n",
        "        \"\"\"\n",
        "        minibatch_size = x.shape[0]\n",
        "        num_features=x.shape[1]\n",
        "        num_classes=y.shape[1]\n",
        "\n",
        "        #print (\"In compute_gradient\")\n",
        "\n",
        "        \n",
        "        #loss matrix to check for 0 values\n",
        "        xw= np.matmul(x,self.w)\n",
        "        loss = 2-xw*y                    #2-xw.y for each element of matrix\n",
        "        loss = loss.clip(min=0)\n",
        "        \n",
        "        \n",
        "        #initialize grad1\n",
        "        grad_1 = np.zeros ((num_features, num_classes))    \n",
        "        x_tr = x.T    \n",
        "        \n",
        "        for jj in range(num_classes):\n",
        "\n",
        "            for ii in range (minibatch_size):\n",
        "\n",
        "                if (loss[ii][jj] > 0.0):\n",
        "\n",
        "                    #if (ii == minibatch_size-1):\n",
        "                        #print (\"stop\")\n",
        "                    #x_row = np.transpose(x[ii,:])\n",
        "                    ###step-1: x_row = x_tr[:,ii]\n",
        "\n",
        "                    ###step-2: term1 = -2*loss[ii][jj]*x_row*y[ii][jj]\n",
        "                    #print (term1.shape)\n",
        "                    #print (term1[num_features-1])\n",
        "                    #x_row_2 += x_row*term1\n",
        "\n",
        "                    grad_1[:,jj] += -2*loss[ii][jj]*y[ii][jj]*x_tr[:,ii]\n",
        "\n",
        "                    #print (grad_1[num_features-1,jj])\n",
        "                else:\n",
        "                    pass\n",
        "              \n",
        "\n",
        "        #Add term2 to the gradients\n",
        "        grad_2 = self.C*(self.w)\n",
        "        #print (grad_2.shape)\n",
        "\n",
        "        #Add 2 terms and divide by n\n",
        "        grad = (grad_1 )/(minibatch_size) + grad_2\n",
        "        \n",
        "        return grad\n",
        "\n",
        "   \n",
        "\n",
        "    def compute_gradient_ok_locally_but_fails_on_gradescope_dont_use (self, x, y):\n",
        "        \"\"\"\n",
        "        x : numpy array of shape (minibatch size, num_features)\n",
        "        y : numpy array of shape (minibatch size, num_classes)\n",
        "        returns : numpy array of shape (num_features, num_classes)\n",
        "        \"\"\"\n",
        "        minibatch_size = x.shape[0]\n",
        "        num_features=x.shape[1]\n",
        "        num_classes=y.shape[1]\n",
        "\n",
        "        #loss matrix to check for 0 values\n",
        "        xw= np.matmul(x,self.w)\n",
        "        loss = 2-xw*y                    #2-xw.y for each element of matrix\n",
        "\n",
        "        loss_copy = loss.clip(min=0)\n",
        "        lossval = np.sum(loss_copy)\n",
        "\n",
        "        #initialize grad1\n",
        "        grad_1 = np.zeros ((num_features, num_classes))\n",
        "        #x_tr = -(x.T)\n",
        "\n",
        "        for ii in range(num_classes):\n",
        "            print (\"Gradient for class:\", ii)\n",
        "            y_row = y[:,ii]\n",
        "            #print (y_row.shape)\n",
        "            #print (x_tr.shape)\n",
        "            t2 = -(x.T)*y_row\n",
        "            #print (t2.shape)\n",
        "            t2sum = np.sum(t2,axis=1)\n",
        "            #print (t2sum.shape)\n",
        "            #todo - check if t2 all columns should be added to create 1 col which goes as deriv[1]\n",
        "            grad_1[:,ii] += t2sum  #TODO - check syntax\n",
        "        \n",
        "        for kk in range(num_features):\n",
        "            for jj in range (num_classes):\n",
        "                if (loss[kk][jj] < 0.0):\n",
        "                    grad_1[kk][jj] = 0.0\n",
        "        \n",
        "        #grad = 2*L*dL/dw (MSE gradient) and hence we do the following, \n",
        "        grad_1 = 2*lossval*grad_1\n",
        "\n",
        "        #Add term2 to the gradients\n",
        "        grad_2 = self.C*(self.w)\n",
        "        #print (grad_2.shape)\n",
        "\n",
        "        #Add 2 terms and divide by n\n",
        "        grad = (grad_1 + grad_2)/(minibatch_size)\n",
        "        \n",
        "        return grad\n",
        "\n",
        "\n",
        "    def compute_gradient_slow_dont_use_me (self, x, y):\n",
        "        \"\"\"\n",
        "        x : numpy array of shape (minibatch size, num_features)\n",
        "        y : numpy array of shape (minibatch size, num_classes)\n",
        "        returns : numpy array of shape (num_features, num_classes)\n",
        "        \"\"\"\n",
        "        xw= np.matmul(x,self.w)\n",
        "        xwy = xw*y\n",
        "        loss = 2 - xwy   #this loss is computed to ensure no contribution made for negative nums \n",
        "\n",
        "        #compute loss fn value (without using square)\n",
        "        loss_copy = loss.clip(min=0)\n",
        "        lossval = np.sum(loss_copy)\n",
        "\n",
        "        num_examples = x.shape[0]\n",
        "        num_features = x.shape[1]\n",
        "        num_classes = y.shape[1] \n",
        "        \n",
        "        #following formula is for grad_a = -x.y, how will this change for our example??\n",
        "        grad_a = np.zeros ([num_features, num_classes])\n",
        "\n",
        "        for ii in range(num_classes):   #doing 1 class at a time\n",
        "\n",
        "            for kk in range (num_examples):   #over all training examples\n",
        "\n",
        "                for jj in range (num_features):  # over all features\n",
        "\n",
        "                    if (loss[kk][ii] > 0.0):                  # (2- xw.y=0)\n",
        "                       grad_a[jj][ii]+= -x[kk][jj]*y[kk][ii]    # y value of ii \n",
        "                    else:\n",
        "                        grad_a[jj][ii]+=0.0\n",
        "\n",
        "        #Now take elementwise product of Loss & Grad1\n",
        "        #grad = 2*L*dL/dw (MSE gradient) and hence we do the following, \n",
        "        grad1 = 2*lossval*grad_a\n",
        "\n",
        "        #Add term2 to the gradients\n",
        "        grad2 = self.C*(self.w)\n",
        "        print (grad2.shape)\n",
        "\n",
        "        #Add 2 terms and divide by n\n",
        "        grad = (grad1 + grad2)/(num_examples)\n",
        "        \n",
        "        return grad\n",
        "\n",
        "\n",
        "\n",
        "    # Batcher function\n",
        "    def minibatch(self, iterable1, iterable2, size=1):\n",
        "        l = len(iterable1)\n",
        "        n = size\n",
        "        for ndx in range(0, l, n):\n",
        "            index2 = min(ndx + n, l)\n",
        "            yield iterable1[ndx: index2], iterable2[ndx: index2]\n",
        "\n",
        "    def infer(self, x):\n",
        "        \"\"\"\n",
        "        x : numpy array of shape (num_examples_to_infer, num_features)\n",
        "        returns : numpy array of shape (num_examples_to_infer, num_classes)\n",
        "        \"\"\"\n",
        "        xw= np.matmul(x,self.w)\n",
        "        class_per_example=np.argmax(xw, axis = 1)\n",
        "        inf = self.make_one_versus_all_labels(class_per_example, self.m)\n",
        "        return inf\n",
        "\n",
        "        pass\n",
        "\n",
        "    def compute_accuracy(self, y_inferred, y):\n",
        "        \"\"\"\n",
        "        y_inferred : numpy array of shape (num_examples, num_classes)\n",
        "        y : numpy array of shape (num_examples, num_classes)\n",
        "        returns : float\n",
        "        \"\"\"\n",
        "        no_examples = y.shape[0]\n",
        "\n",
        "        actual_y=np.argmax(y, axis = 1)\n",
        "        inferred_y=np.argmax(y_inferred, axis = 1)\n",
        "\n",
        "        assert (len(actual_y)== no_examples)\n",
        "        assert (len(inferred_y)== no_examples)\n",
        "\n",
        "\n",
        "        no_correct=np.sum(actual_y == inferred_y)\n",
        "\n",
        "        accuracy = no_correct / no_examples\n",
        "\n",
        "        return accuracy\n",
        "        pass\n",
        "\n",
        "    def fit(self, x_train, y_train, x_test, y_test):\n",
        "        \"\"\"\n",
        "        x_train : numpy array of shape (number of training examples, num_features)\n",
        "        y_train : numpy array of shape (number of training examples, num_classes)\n",
        "        x_test : numpy array of shape (number of training examples, nujm_features)\n",
        "        y_test : numpy array of shape (number of training examples, num_classes)\n",
        "        returns : float, float, float, float\n",
        "        \"\"\"\n",
        "        self.num_features = x_train.shape[1]\n",
        "        self.m = y_train.max() + 1\n",
        "        y_train = self.make_one_versus_all_labels(y_train, self.m)\n",
        "        y_test = self.make_one_versus_all_labels(y_test, self.m)\n",
        "        self.w = np.zeros([self.num_features, self.m])\n",
        "\n",
        "        train_losses = []\n",
        "        train_accs = []\n",
        "        test_losses = []\n",
        "        test_accs = []\n",
        "\n",
        "        for iteration in range(self.niter):\n",
        "            # Train one pass through the training set\n",
        "            for x, y in self.minibatch(x_train, y_train, size=self.batch_size):\n",
        "                grad = self.compute_gradient(x, y)\n",
        "                self.w -= self.eta * grad\n",
        "\n",
        "            # Measure loss and accuracy on training set\n",
        "            train_loss = self.compute_loss(x_train, y_train)\n",
        "            y_inferred = self.infer(x_train)\n",
        "            train_accuracy = self.compute_accuracy(y_inferred, y_train)\n",
        "\n",
        "            # Measure loss and accuracy on test set\n",
        "            test_loss = self.compute_loss(x_test, y_test)\n",
        "            y_inferred = self.infer(x_test)\n",
        "            test_accuracy = self.compute_accuracy(y_inferred, y_test)\n",
        "\n",
        "            if self.verbose:\n",
        "                print (\"iter=\", iteration)\n",
        "                print(f\"Iteration {iteration} | Train loss {train_loss:.04f} | Train acc {train_accuracy:.04f} |\"\n",
        "                      f\" Test loss {test_loss:.04f} | Test acc {test_accuracy:.04f}\")\n",
        "\n",
        "            # Record losses, accs\n",
        "            train_losses.append(train_loss)\n",
        "            train_accs.append(train_accuracy)\n",
        "            test_losses.append(test_loss)\n",
        "            test_accs.append(test_accuracy)\n",
        "\n",
        "        return train_losses, train_accs, test_losses, test_accs\n",
        "\n",
        "\n",
        "# DO NOT MODIFY THIS FUNCTION\n",
        "def load_data():\n",
        "    # Load the data files\n",
        "    print(\"Loading data...\")\n",
        "    x_train = np.load(\"train_features_cifar100_reduced.npz\")[\"train_data\"]\n",
        "    x_test = np.load(\"test_features_cifar100_reduced.npz\")[\"test_data\"]\n",
        "    y_train = np.load(\"train_labels_cifar100_reduced.npz\")[\"train_label\"]\n",
        "    y_test = np.load(\"test_labels_cifar100_reduced.npz\")[\"test_label\"]\n",
        "\n",
        "    # normalize the data\n",
        "    mean = x_train.mean(axis=0)\n",
        "    std = x_train.std(axis=0)\n",
        "    x_train = (x_train - mean) / std\n",
        "    x_test = (x_test - mean) / std\n",
        "\n",
        "    # add implicit bias in the feature\n",
        "    x_train = np.concatenate([x_train, np.ones((x_train.shape[0], 1))], axis=1)\n",
        "    x_test = np.concatenate([x_test, np.ones((x_test.shape[0], 1))], axis=1)\n",
        "\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    x_train, y_train, x_test, y_test = load_data()\n",
        "    \n",
        "    print(\"Fitting the model...\")\n",
        "    svm = SVM(eta=0.0001, C=1, niter=200, batch_size=5000, verbose=True)\n",
        "    train_losses, train_accs, test_losses, test_accs = svm.fit(x_train, y_train, x_test, y_test)\n",
        "\n",
        "    # # to infer after training, do the following:\n",
        "    y_inferred = svm.infer(x_test)\n",
        "    y_test_ova = svm.make_one_versus_all_labels(y_test, 8)\n",
        "    ## to compute the gradient or loss before training, do the following:\n",
        "    oldTestCode=False\n",
        "\n",
        "    if (oldTestCode):\n",
        "        y_train_ova = svm.make_one_versus_all_labels(y_train, 8) # one-versus-all labels\n",
        "        svm.w = np.zeros([3073, 8])\n",
        "        \n",
        "        loss = svm.compute_loss(x_train, y_train_ova)\n",
        "        grad = svm.compute_gradient(x_train, y_train_ova)\n",
        "\n",
        "    print (\"done\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Fitting the model...\n",
            "iter= 0\n",
            "Iteration 0 | Train loss 31.5301 | Train acc 0.3036 | Test loss 31.5436 | Test acc 0.2960\n",
            "iter= 1\n",
            "Iteration 1 | Train loss 31.2682 | Train acc 0.3212 | Test loss 31.2893 | Test acc 0.3125\n",
            "iter= 2\n",
            "Iteration 2 | Train loss 31.0809 | Train acc 0.3311 | Test loss 31.1074 | Test acc 0.3222\n",
            "iter= 3\n",
            "Iteration 3 | Train loss 30.9356 | Train acc 0.3366 | Test loss 30.9668 | Test acc 0.3280\n",
            "iter= 4\n",
            "Iteration 4 | Train loss 30.8180 | Train acc 0.3432 | Test loss 30.8536 | Test acc 0.3347\n",
            "iter= 5\n",
            "Iteration 5 | Train loss 30.7199 | Train acc 0.3485 | Test loss 30.7596 | Test acc 0.3410\n",
            "iter= 6\n",
            "Iteration 6 | Train loss 30.6355 | Train acc 0.3513 | Test loss 30.6791 | Test acc 0.3448\n",
            "iter= 7\n",
            "Iteration 7 | Train loss 30.5611 | Train acc 0.3549 | Test loss 30.6083 | Test acc 0.3488\n",
            "iter= 8\n",
            "Iteration 8 | Train loss 30.4939 | Train acc 0.3596 | Test loss 30.5445 | Test acc 0.3510\n",
            "iter= 9\n",
            "Iteration 9 | Train loss 30.4321 | Train acc 0.3619 | Test loss 30.4859 | Test acc 0.3523\n",
            "iter= 10\n",
            "Iteration 10 | Train loss 30.3745 | Train acc 0.3644 | Test loss 30.4313 | Test acc 0.3515\n",
            "iter= 11\n",
            "Iteration 11 | Train loss 30.3202 | Train acc 0.3670 | Test loss 30.3798 | Test acc 0.3530\n",
            "iter= 12\n",
            "Iteration 12 | Train loss 30.2685 | Train acc 0.3696 | Test loss 30.3308 | Test acc 0.3550\n",
            "iter= 13\n",
            "Iteration 13 | Train loss 30.2190 | Train acc 0.3716 | Test loss 30.2837 | Test acc 0.3578\n",
            "iter= 14\n",
            "Iteration 14 | Train loss 30.1712 | Train acc 0.3739 | Test loss 30.2384 | Test acc 0.3598\n",
            "iter= 15\n",
            "Iteration 15 | Train loss 30.1250 | Train acc 0.3760 | Test loss 30.1944 | Test acc 0.3618\n",
            "iter= 16\n",
            "Iteration 16 | Train loss 30.0801 | Train acc 0.3789 | Test loss 30.1517 | Test acc 0.3643\n",
            "iter= 17\n",
            "Iteration 17 | Train loss 30.0364 | Train acc 0.3800 | Test loss 30.1101 | Test acc 0.3658\n",
            "iter= 18\n",
            "Iteration 18 | Train loss 29.9938 | Train acc 0.3820 | Test loss 30.0695 | Test acc 0.3673\n",
            "iter= 19\n",
            "Iteration 19 | Train loss 29.9521 | Train acc 0.3836 | Test loss 30.0297 | Test acc 0.3700\n",
            "iter= 20\n",
            "Iteration 20 | Train loss 29.9113 | Train acc 0.3849 | Test loss 29.9907 | Test acc 0.3725\n",
            "iter= 21\n",
            "Iteration 21 | Train loss 29.8713 | Train acc 0.3861 | Test loss 29.9525 | Test acc 0.3728\n",
            "iter= 22\n",
            "Iteration 22 | Train loss 29.8320 | Train acc 0.3862 | Test loss 29.9150 | Test acc 0.3735\n",
            "iter= 23\n",
            "Iteration 23 | Train loss 29.7934 | Train acc 0.3868 | Test loss 29.8781 | Test acc 0.3740\n",
            "iter= 24\n",
            "Iteration 24 | Train loss 29.7554 | Train acc 0.3878 | Test loss 29.8417 | Test acc 0.3762\n",
            "iter= 25\n",
            "Iteration 25 | Train loss 29.7180 | Train acc 0.3890 | Test loss 29.8060 | Test acc 0.3780\n",
            "iter= 26\n",
            "Iteration 26 | Train loss 29.6811 | Train acc 0.3901 | Test loss 29.7707 | Test acc 0.3800\n",
            "iter= 27\n",
            "Iteration 27 | Train loss 29.6448 | Train acc 0.3912 | Test loss 29.7359 | Test acc 0.3792\n",
            "iter= 28\n",
            "Iteration 28 | Train loss 29.6089 | Train acc 0.3923 | Test loss 29.7015 | Test acc 0.3807\n",
            "iter= 29\n",
            "Iteration 29 | Train loss 29.5736 | Train acc 0.3930 | Test loss 29.6676 | Test acc 0.3822\n",
            "iter= 30\n",
            "Iteration 30 | Train loss 29.5386 | Train acc 0.3941 | Test loss 29.6341 | Test acc 0.3820\n",
            "iter= 31\n",
            "Iteration 31 | Train loss 29.5040 | Train acc 0.3947 | Test loss 29.6009 | Test acc 0.3830\n",
            "iter= 32\n",
            "Iteration 32 | Train loss 29.4698 | Train acc 0.3955 | Test loss 29.5681 | Test acc 0.3837\n",
            "iter= 33\n",
            "Iteration 33 | Train loss 29.4360 | Train acc 0.3962 | Test loss 29.5357 | Test acc 0.3837\n",
            "iter= 34\n",
            "Iteration 34 | Train loss 29.4026 | Train acc 0.3966 | Test loss 29.5035 | Test acc 0.3850\n",
            "iter= 35\n",
            "Iteration 35 | Train loss 29.3694 | Train acc 0.3972 | Test loss 29.4717 | Test acc 0.3852\n",
            "iter= 36\n",
            "Iteration 36 | Train loss 29.3366 | Train acc 0.3976 | Test loss 29.4402 | Test acc 0.3857\n",
            "iter= 37\n",
            "Iteration 37 | Train loss 29.3040 | Train acc 0.3982 | Test loss 29.4089 | Test acc 0.3855\n",
            "iter= 38\n",
            "Iteration 38 | Train loss 29.2718 | Train acc 0.3988 | Test loss 29.3779 | Test acc 0.3855\n",
            "iter= 39\n",
            "Iteration 39 | Train loss 29.2398 | Train acc 0.3988 | Test loss 29.3471 | Test acc 0.3850\n",
            "iter= 40\n",
            "Iteration 40 | Train loss 29.2081 | Train acc 0.3995 | Test loss 29.3166 | Test acc 0.3865\n",
            "iter= 41\n",
            "Iteration 41 | Train loss 29.1766 | Train acc 0.3997 | Test loss 29.2864 | Test acc 0.3870\n",
            "iter= 42\n",
            "Iteration 42 | Train loss 29.1454 | Train acc 0.4008 | Test loss 29.2563 | Test acc 0.3862\n",
            "iter= 43\n",
            "Iteration 43 | Train loss 29.1144 | Train acc 0.4017 | Test loss 29.2265 | Test acc 0.3860\n",
            "iter= 44\n",
            "Iteration 44 | Train loss 29.0836 | Train acc 0.4022 | Test loss 29.1968 | Test acc 0.3850\n",
            "iter= 45\n",
            "Iteration 45 | Train loss 29.0531 | Train acc 0.4030 | Test loss 29.1674 | Test acc 0.3855\n",
            "iter= 46\n",
            "Iteration 46 | Train loss 29.0227 | Train acc 0.4029 | Test loss 29.1381 | Test acc 0.3865\n",
            "iter= 47\n",
            "Iteration 47 | Train loss 28.9926 | Train acc 0.4035 | Test loss 29.1091 | Test acc 0.3867\n",
            "iter= 48\n",
            "Iteration 48 | Train loss 28.9626 | Train acc 0.4042 | Test loss 29.0802 | Test acc 0.3865\n",
            "iter= 49\n",
            "Iteration 49 | Train loss 28.9328 | Train acc 0.4042 | Test loss 29.0514 | Test acc 0.3867\n",
            "iter= 50\n",
            "Iteration 50 | Train loss 28.9032 | Train acc 0.4051 | Test loss 29.0229 | Test acc 0.3887\n",
            "iter= 51\n",
            "Iteration 51 | Train loss 28.8738 | Train acc 0.4054 | Test loss 28.9945 | Test acc 0.3902\n",
            "iter= 52\n",
            "Iteration 52 | Train loss 28.8446 | Train acc 0.4055 | Test loss 28.9663 | Test acc 0.3902\n",
            "iter= 53\n",
            "Iteration 53 | Train loss 28.8155 | Train acc 0.4058 | Test loss 28.9382 | Test acc 0.3900\n",
            "iter= 54\n",
            "Iteration 54 | Train loss 28.7865 | Train acc 0.4063 | Test loss 28.9103 | Test acc 0.3905\n",
            "iter= 55\n",
            "Iteration 55 | Train loss 28.7578 | Train acc 0.4066 | Test loss 28.8825 | Test acc 0.3907\n",
            "iter= 56\n",
            "Iteration 56 | Train loss 28.7291 | Train acc 0.4069 | Test loss 28.8548 | Test acc 0.3912\n",
            "iter= 57\n",
            "Iteration 57 | Train loss 28.7007 | Train acc 0.4068 | Test loss 28.8273 | Test acc 0.3915\n",
            "iter= 58\n",
            "Iteration 58 | Train loss 28.6723 | Train acc 0.4069 | Test loss 28.7999 | Test acc 0.3920\n",
            "iter= 59\n",
            "Iteration 59 | Train loss 28.6441 | Train acc 0.4071 | Test loss 28.7726 | Test acc 0.3915\n",
            "iter= 60\n",
            "Iteration 60 | Train loss 28.6161 | Train acc 0.4073 | Test loss 28.7455 | Test acc 0.3910\n",
            "iter= 61\n",
            "Iteration 61 | Train loss 28.5881 | Train acc 0.4076 | Test loss 28.7185 | Test acc 0.3912\n",
            "iter= 62\n",
            "Iteration 62 | Train loss 28.5603 | Train acc 0.4077 | Test loss 28.6916 | Test acc 0.3915\n",
            "iter= 63\n",
            "Iteration 63 | Train loss 28.5327 | Train acc 0.4079 | Test loss 28.6649 | Test acc 0.3915\n",
            "iter= 64\n",
            "Iteration 64 | Train loss 28.5051 | Train acc 0.4075 | Test loss 28.6382 | Test acc 0.3917\n",
            "iter= 65\n",
            "Iteration 65 | Train loss 28.4777 | Train acc 0.4076 | Test loss 28.6117 | Test acc 0.3912\n",
            "iter= 66\n",
            "Iteration 66 | Train loss 28.4504 | Train acc 0.4082 | Test loss 28.5852 | Test acc 0.3907\n",
            "iter= 67\n",
            "Iteration 67 | Train loss 28.4232 | Train acc 0.4086 | Test loss 28.5589 | Test acc 0.3907\n",
            "iter= 68\n",
            "Iteration 68 | Train loss 28.3961 | Train acc 0.4092 | Test loss 28.5327 | Test acc 0.3907\n",
            "iter= 69\n",
            "Iteration 69 | Train loss 28.3692 | Train acc 0.4096 | Test loss 28.5066 | Test acc 0.3915\n",
            "iter= 70\n",
            "Iteration 70 | Train loss 28.3423 | Train acc 0.4097 | Test loss 28.4806 | Test acc 0.3915\n",
            "iter= 71\n",
            "Iteration 71 | Train loss 28.3156 | Train acc 0.4102 | Test loss 28.4547 | Test acc 0.3917\n",
            "iter= 72\n",
            "Iteration 72 | Train loss 28.2889 | Train acc 0.4103 | Test loss 28.4289 | Test acc 0.3922\n",
            "iter= 73\n",
            "Iteration 73 | Train loss 28.2624 | Train acc 0.4106 | Test loss 28.4031 | Test acc 0.3932\n",
            "iter= 74\n",
            "Iteration 74 | Train loss 28.2360 | Train acc 0.4111 | Test loss 28.3775 | Test acc 0.3932\n",
            "iter= 75\n",
            "Iteration 75 | Train loss 28.2096 | Train acc 0.4113 | Test loss 28.3520 | Test acc 0.3940\n",
            "iter= 76\n",
            "Iteration 76 | Train loss 28.1834 | Train acc 0.4115 | Test loss 28.3266 | Test acc 0.3937\n",
            "iter= 77\n",
            "Iteration 77 | Train loss 28.1573 | Train acc 0.4116 | Test loss 28.3012 | Test acc 0.3940\n",
            "iter= 78\n",
            "Iteration 78 | Train loss 28.1313 | Train acc 0.4121 | Test loss 28.2760 | Test acc 0.3940\n",
            "iter= 79\n",
            "Iteration 79 | Train loss 28.1053 | Train acc 0.4121 | Test loss 28.2508 | Test acc 0.3940\n",
            "iter= 80\n",
            "Iteration 80 | Train loss 28.0795 | Train acc 0.4128 | Test loss 28.2258 | Test acc 0.3940\n",
            "iter= 81\n",
            "Iteration 81 | Train loss 28.0537 | Train acc 0.4132 | Test loss 28.2008 | Test acc 0.3947\n",
            "iter= 82\n",
            "Iteration 82 | Train loss 28.0281 | Train acc 0.4132 | Test loss 28.1759 | Test acc 0.3952\n",
            "iter= 83\n",
            "Iteration 83 | Train loss 28.0025 | Train acc 0.4138 | Test loss 28.1511 | Test acc 0.3952\n",
            "iter= 84\n",
            "Iteration 84 | Train loss 27.9770 | Train acc 0.4143 | Test loss 28.1263 | Test acc 0.3952\n",
            "iter= 85\n",
            "Iteration 85 | Train loss 27.9517 | Train acc 0.4145 | Test loss 28.1017 | Test acc 0.3960\n",
            "iter= 86\n",
            "Iteration 86 | Train loss 27.9263 | Train acc 0.4148 | Test loss 28.0771 | Test acc 0.3970\n",
            "iter= 87\n",
            "Iteration 87 | Train loss 27.9011 | Train acc 0.4148 | Test loss 28.0526 | Test acc 0.3972\n",
            "iter= 88\n",
            "Iteration 88 | Train loss 27.8760 | Train acc 0.4150 | Test loss 28.0282 | Test acc 0.3975\n",
            "iter= 89\n",
            "Iteration 89 | Train loss 27.8510 | Train acc 0.4153 | Test loss 28.0039 | Test acc 0.3972\n",
            "iter= 90\n",
            "Iteration 90 | Train loss 27.8260 | Train acc 0.4155 | Test loss 27.9797 | Test acc 0.3970\n",
            "iter= 91\n",
            "Iteration 91 | Train loss 27.8011 | Train acc 0.4158 | Test loss 27.9555 | Test acc 0.3972\n",
            "iter= 92\n",
            "Iteration 92 | Train loss 27.7763 | Train acc 0.4160 | Test loss 27.9314 | Test acc 0.3972\n",
            "iter= 93\n",
            "Iteration 93 | Train loss 27.7516 | Train acc 0.4161 | Test loss 27.9074 | Test acc 0.3975\n",
            "iter= 94\n",
            "Iteration 94 | Train loss 27.7270 | Train acc 0.4160 | Test loss 27.8834 | Test acc 0.3975\n",
            "iter= 95\n",
            "Iteration 95 | Train loss 27.7024 | Train acc 0.4162 | Test loss 27.8596 | Test acc 0.3967\n",
            "iter= 96\n",
            "Iteration 96 | Train loss 27.6779 | Train acc 0.4162 | Test loss 27.8358 | Test acc 0.3965\n",
            "iter= 97\n",
            "Iteration 97 | Train loss 27.6535 | Train acc 0.4162 | Test loss 27.8121 | Test acc 0.3965\n",
            "iter= 98\n",
            "Iteration 98 | Train loss 27.6292 | Train acc 0.4169 | Test loss 27.7884 | Test acc 0.3967\n",
            "iter= 99\n",
            "Iteration 99 | Train loss 27.6050 | Train acc 0.4168 | Test loss 27.7648 | Test acc 0.3967\n",
            "iter= 100\n",
            "Iteration 100 | Train loss 27.5808 | Train acc 0.4169 | Test loss 27.7413 | Test acc 0.3967\n",
            "iter= 101\n",
            "Iteration 101 | Train loss 27.5567 | Train acc 0.4172 | Test loss 27.7179 | Test acc 0.3970\n",
            "iter= 102\n",
            "Iteration 102 | Train loss 27.5327 | Train acc 0.4173 | Test loss 27.6945 | Test acc 0.3972\n",
            "iter= 103\n",
            "Iteration 103 | Train loss 27.5087 | Train acc 0.4172 | Test loss 27.6712 | Test acc 0.3982\n",
            "iter= 104\n",
            "Iteration 104 | Train loss 27.4848 | Train acc 0.4174 | Test loss 27.6480 | Test acc 0.3980\n",
            "iter= 105\n",
            "Iteration 105 | Train loss 27.4610 | Train acc 0.4174 | Test loss 27.6248 | Test acc 0.3975\n",
            "iter= 106\n",
            "Iteration 106 | Train loss 27.4373 | Train acc 0.4174 | Test loss 27.6017 | Test acc 0.3970\n",
            "iter= 107\n",
            "Iteration 107 | Train loss 27.4136 | Train acc 0.4174 | Test loss 27.5787 | Test acc 0.3970\n",
            "iter= 108\n",
            "Iteration 108 | Train loss 27.3900 | Train acc 0.4176 | Test loss 27.5557 | Test acc 0.3967\n",
            "iter= 109\n",
            "Iteration 109 | Train loss 27.3665 | Train acc 0.4177 | Test loss 27.5329 | Test acc 0.3970\n",
            "iter= 110\n",
            "Iteration 110 | Train loss 27.3431 | Train acc 0.4179 | Test loss 27.5100 | Test acc 0.3975\n",
            "iter= 111\n",
            "Iteration 111 | Train loss 27.3197 | Train acc 0.4178 | Test loss 27.4873 | Test acc 0.3975\n",
            "iter= 112\n",
            "Iteration 112 | Train loss 27.2964 | Train acc 0.4176 | Test loss 27.4646 | Test acc 0.3972\n",
            "iter= 113\n",
            "Iteration 113 | Train loss 27.2731 | Train acc 0.4178 | Test loss 27.4419 | Test acc 0.3977\n",
            "iter= 114\n",
            "Iteration 114 | Train loss 27.2500 | Train acc 0.4180 | Test loss 27.4194 | Test acc 0.3970\n",
            "iter= 115\n",
            "Iteration 115 | Train loss 27.2269 | Train acc 0.4182 | Test loss 27.3969 | Test acc 0.3970\n",
            "iter= 116\n",
            "Iteration 116 | Train loss 27.2038 | Train acc 0.4182 | Test loss 27.3744 | Test acc 0.3972\n",
            "iter= 117\n",
            "Iteration 117 | Train loss 27.1808 | Train acc 0.4183 | Test loss 27.3520 | Test acc 0.3970\n",
            "iter= 118\n",
            "Iteration 118 | Train loss 27.1579 | Train acc 0.4181 | Test loss 27.3297 | Test acc 0.3975\n",
            "iter= 119\n",
            "Iteration 119 | Train loss 27.1351 | Train acc 0.4183 | Test loss 27.3075 | Test acc 0.3982\n",
            "iter= 120\n",
            "Iteration 120 | Train loss 27.1123 | Train acc 0.4183 | Test loss 27.2853 | Test acc 0.3985\n",
            "iter= 121\n",
            "Iteration 121 | Train loss 27.0896 | Train acc 0.4185 | Test loss 27.2632 | Test acc 0.3987\n",
            "iter= 122\n",
            "Iteration 122 | Train loss 27.0669 | Train acc 0.4184 | Test loss 27.2411 | Test acc 0.3987\n",
            "iter= 123\n",
            "Iteration 123 | Train loss 27.0444 | Train acc 0.4187 | Test loss 27.2191 | Test acc 0.3995\n",
            "iter= 124\n",
            "Iteration 124 | Train loss 27.0218 | Train acc 0.4187 | Test loss 27.1971 | Test acc 0.4000\n",
            "iter= 125\n",
            "Iteration 125 | Train loss 26.9994 | Train acc 0.4189 | Test loss 27.1753 | Test acc 0.3995\n",
            "iter= 126\n",
            "Iteration 126 | Train loss 26.9770 | Train acc 0.4190 | Test loss 27.1534 | Test acc 0.3992\n",
            "iter= 127\n",
            "Iteration 127 | Train loss 26.9547 | Train acc 0.4194 | Test loss 27.1317 | Test acc 0.3997\n",
            "iter= 128\n",
            "Iteration 128 | Train loss 26.9324 | Train acc 0.4197 | Test loss 27.1100 | Test acc 0.4000\n",
            "iter= 129\n",
            "Iteration 129 | Train loss 26.9102 | Train acc 0.4198 | Test loss 27.0883 | Test acc 0.4007\n",
            "iter= 130\n",
            "Iteration 130 | Train loss 26.8880 | Train acc 0.4200 | Test loss 27.0667 | Test acc 0.4010\n",
            "iter= 131\n",
            "Iteration 131 | Train loss 26.8660 | Train acc 0.4200 | Test loss 27.0452 | Test acc 0.4005\n",
            "iter= 132\n",
            "Iteration 132 | Train loss 26.8439 | Train acc 0.4202 | Test loss 27.0237 | Test acc 0.4010\n",
            "iter= 133\n",
            "Iteration 133 | Train loss 26.8220 | Train acc 0.4204 | Test loss 27.0023 | Test acc 0.4010\n",
            "iter= 134\n",
            "Iteration 134 | Train loss 26.8001 | Train acc 0.4204 | Test loss 26.9810 | Test acc 0.4015\n",
            "iter= 135\n",
            "Iteration 135 | Train loss 26.7782 | Train acc 0.4207 | Test loss 26.9597 | Test acc 0.4017\n",
            "iter= 136\n",
            "Iteration 136 | Train loss 26.7564 | Train acc 0.4209 | Test loss 26.9384 | Test acc 0.4015\n",
            "iter= 137\n",
            "Iteration 137 | Train loss 26.7347 | Train acc 0.4210 | Test loss 26.9172 | Test acc 0.4017\n",
            "iter= 138\n",
            "Iteration 138 | Train loss 26.7131 | Train acc 0.4212 | Test loss 26.8961 | Test acc 0.4017\n",
            "iter= 139\n",
            "Iteration 139 | Train loss 26.6915 | Train acc 0.4213 | Test loss 26.8750 | Test acc 0.4020\n",
            "iter= 140\n",
            "Iteration 140 | Train loss 26.6699 | Train acc 0.4213 | Test loss 26.8540 | Test acc 0.4025\n",
            "iter= 141\n",
            "Iteration 141 | Train loss 26.6484 | Train acc 0.4214 | Test loss 26.8331 | Test acc 0.4025\n",
            "iter= 142\n",
            "Iteration 142 | Train loss 26.6270 | Train acc 0.4213 | Test loss 26.8122 | Test acc 0.4025\n",
            "iter= 143\n",
            "Iteration 143 | Train loss 26.6056 | Train acc 0.4214 | Test loss 26.7913 | Test acc 0.4025\n",
            "iter= 144\n",
            "Iteration 144 | Train loss 26.5843 | Train acc 0.4214 | Test loss 26.7705 | Test acc 0.4030\n",
            "iter= 145\n",
            "Iteration 145 | Train loss 26.5631 | Train acc 0.4217 | Test loss 26.7498 | Test acc 0.4027\n",
            "iter= 146\n",
            "Iteration 146 | Train loss 26.5419 | Train acc 0.4219 | Test loss 26.7291 | Test acc 0.4027\n",
            "iter= 147\n",
            "Iteration 147 | Train loss 26.5207 | Train acc 0.4219 | Test loss 26.7085 | Test acc 0.4030\n",
            "iter= 148\n",
            "Iteration 148 | Train loss 26.4996 | Train acc 0.4219 | Test loss 26.6879 | Test acc 0.4027\n",
            "iter= 149\n",
            "Iteration 149 | Train loss 26.4786 | Train acc 0.4220 | Test loss 26.6674 | Test acc 0.4027\n",
            "iter= 150\n",
            "Iteration 150 | Train loss 26.4576 | Train acc 0.4220 | Test loss 26.6469 | Test acc 0.4027\n",
            "iter= 151\n",
            "Iteration 151 | Train loss 26.4367 | Train acc 0.4220 | Test loss 26.6265 | Test acc 0.4025\n",
            "iter= 152\n",
            "Iteration 152 | Train loss 26.4159 | Train acc 0.4221 | Test loss 26.6061 | Test acc 0.4025\n",
            "iter= 153\n",
            "Iteration 153 | Train loss 26.3951 | Train acc 0.4222 | Test loss 26.5858 | Test acc 0.4027\n",
            "iter= 154\n",
            "Iteration 154 | Train loss 26.3743 | Train acc 0.4220 | Test loss 26.5656 | Test acc 0.4025\n",
            "iter= 155\n",
            "Iteration 155 | Train loss 26.3536 | Train acc 0.4220 | Test loss 26.5454 | Test acc 0.4022\n",
            "iter= 156\n",
            "Iteration 156 | Train loss 26.3330 | Train acc 0.4220 | Test loss 26.5252 | Test acc 0.4022\n",
            "iter= 157\n",
            "Iteration 157 | Train loss 26.3124 | Train acc 0.4220 | Test loss 26.5051 | Test acc 0.4015\n",
            "iter= 158\n",
            "Iteration 158 | Train loss 26.2919 | Train acc 0.4220 | Test loss 26.4851 | Test acc 0.4017\n",
            "iter= 159\n",
            "Iteration 159 | Train loss 26.2714 | Train acc 0.4221 | Test loss 26.4651 | Test acc 0.4020\n",
            "iter= 160\n",
            "Iteration 160 | Train loss 26.2510 | Train acc 0.4224 | Test loss 26.4452 | Test acc 0.4020\n",
            "iter= 161\n",
            "Iteration 161 | Train loss 26.2306 | Train acc 0.4223 | Test loss 26.4253 | Test acc 0.4020\n",
            "iter= 162\n",
            "Iteration 162 | Train loss 26.2103 | Train acc 0.4223 | Test loss 26.4054 | Test acc 0.4022\n",
            "iter= 163\n",
            "Iteration 163 | Train loss 26.1900 | Train acc 0.4224 | Test loss 26.3857 | Test acc 0.4025\n",
            "iter= 164\n",
            "Iteration 164 | Train loss 26.1698 | Train acc 0.4225 | Test loss 26.3659 | Test acc 0.4030\n",
            "iter= 165\n",
            "Iteration 165 | Train loss 26.1497 | Train acc 0.4224 | Test loss 26.3462 | Test acc 0.4027\n",
            "iter= 166\n",
            "Iteration 166 | Train loss 26.1296 | Train acc 0.4224 | Test loss 26.3266 | Test acc 0.4027\n",
            "iter= 167\n",
            "Iteration 167 | Train loss 26.1095 | Train acc 0.4227 | Test loss 26.3070 | Test acc 0.4020\n",
            "iter= 168\n",
            "Iteration 168 | Train loss 26.0895 | Train acc 0.4227 | Test loss 26.2875 | Test acc 0.4022\n",
            "iter= 169\n",
            "Iteration 169 | Train loss 26.0696 | Train acc 0.4231 | Test loss 26.2680 | Test acc 0.4022\n",
            "iter= 170\n",
            "Iteration 170 | Train loss 26.0497 | Train acc 0.4234 | Test loss 26.2486 | Test acc 0.4022\n",
            "iter= 171\n",
            "Iteration 171 | Train loss 26.0298 | Train acc 0.4237 | Test loss 26.2292 | Test acc 0.4020\n",
            "iter= 172\n",
            "Iteration 172 | Train loss 26.0101 | Train acc 0.4235 | Test loss 26.2099 | Test acc 0.4020\n",
            "iter= 173\n",
            "Iteration 173 | Train loss 25.9903 | Train acc 0.4233 | Test loss 26.1906 | Test acc 0.4022\n",
            "iter= 174\n",
            "Iteration 174 | Train loss 25.9706 | Train acc 0.4233 | Test loss 26.1714 | Test acc 0.4025\n",
            "iter= 175\n",
            "Iteration 175 | Train loss 25.9510 | Train acc 0.4234 | Test loss 26.1522 | Test acc 0.4027\n",
            "iter= 176\n",
            "Iteration 176 | Train loss 25.9314 | Train acc 0.4235 | Test loss 26.1330 | Test acc 0.4025\n",
            "iter= 177\n",
            "Iteration 177 | Train loss 25.9119 | Train acc 0.4238 | Test loss 26.1140 | Test acc 0.4025\n",
            "iter= 178\n",
            "Iteration 178 | Train loss 25.8924 | Train acc 0.4239 | Test loss 26.0949 | Test acc 0.4022\n",
            "iter= 179\n",
            "Iteration 179 | Train loss 25.8730 | Train acc 0.4239 | Test loss 26.0759 | Test acc 0.4020\n",
            "iter= 180\n",
            "Iteration 180 | Train loss 25.8536 | Train acc 0.4239 | Test loss 26.0570 | Test acc 0.4020\n",
            "iter= 181\n",
            "Iteration 181 | Train loss 25.8342 | Train acc 0.4238 | Test loss 26.0381 | Test acc 0.4022\n",
            "iter= 182\n",
            "Iteration 182 | Train loss 25.8150 | Train acc 0.4239 | Test loss 26.0193 | Test acc 0.4025\n",
            "iter= 183\n",
            "Iteration 183 | Train loss 25.7957 | Train acc 0.4239 | Test loss 26.0005 | Test acc 0.4027\n",
            "iter= 184\n",
            "Iteration 184 | Train loss 25.7765 | Train acc 0.4240 | Test loss 25.9817 | Test acc 0.4020\n",
            "iter= 185\n",
            "Iteration 185 | Train loss 25.7574 | Train acc 0.4241 | Test loss 25.9630 | Test acc 0.4015\n",
            "iter= 186\n",
            "Iteration 186 | Train loss 25.7383 | Train acc 0.4241 | Test loss 25.9444 | Test acc 0.4017\n",
            "iter= 187\n",
            "Iteration 187 | Train loss 25.7193 | Train acc 0.4243 | Test loss 25.9258 | Test acc 0.4015\n",
            "iter= 188\n",
            "Iteration 188 | Train loss 25.7003 | Train acc 0.4243 | Test loss 25.9072 | Test acc 0.4015\n",
            "iter= 189\n",
            "Iteration 189 | Train loss 25.6813 | Train acc 0.4245 | Test loss 25.8887 | Test acc 0.4012\n",
            "iter= 190\n",
            "Iteration 190 | Train loss 25.6625 | Train acc 0.4246 | Test loss 25.8702 | Test acc 0.4015\n",
            "iter= 191\n",
            "Iteration 191 | Train loss 25.6436 | Train acc 0.4249 | Test loss 25.8518 | Test acc 0.4017\n",
            "iter= 192\n",
            "Iteration 192 | Train loss 25.6248 | Train acc 0.4251 | Test loss 25.8334 | Test acc 0.4020\n",
            "iter= 193\n",
            "Iteration 193 | Train loss 25.6061 | Train acc 0.4252 | Test loss 25.8151 | Test acc 0.4022\n",
            "iter= 194\n",
            "Iteration 194 | Train loss 25.5874 | Train acc 0.4251 | Test loss 25.7968 | Test acc 0.4022\n",
            "iter= 195\n",
            "Iteration 195 | Train loss 25.5687 | Train acc 0.4252 | Test loss 25.7786 | Test acc 0.4025\n",
            "iter= 196\n",
            "Iteration 196 | Train loss 25.5501 | Train acc 0.4254 | Test loss 25.7604 | Test acc 0.4020\n",
            "iter= 197\n",
            "Iteration 197 | Train loss 25.5316 | Train acc 0.4254 | Test loss 25.7423 | Test acc 0.4020\n",
            "iter= 198\n",
            "Iteration 198 | Train loss 25.5130 | Train acc 0.4255 | Test loss 25.7242 | Test acc 0.4020\n",
            "iter= 199\n",
            "Iteration 199 | Train loss 25.4946 | Train acc 0.4255 | Test loss 25.7061 | Test acc 0.4025\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}