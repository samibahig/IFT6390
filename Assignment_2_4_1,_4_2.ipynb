{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 2: 4.1, 4.2",
      "provenance": [],
      "authorship_tag": "ABX9TyOH+p9MFR9a0Q52MDR0S60j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samibahig/IFT6390/blob/main/Assignment_2_4_1%2C_4_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iwwYIJVZN0C"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import math, copy\n",
        "import pickle\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def clones(module, N):\n",
        "    \"\"\"\n",
        "    A helper function for producing N identical layers (each with their own parameters).\n",
        "\n",
        "    inputs:\n",
        "        module: a pytorch nn.module\n",
        "        N (int): the number of copies of that module to return\n",
        "\n",
        "    returns:\n",
        "        a ModuleList with the copies of the module (the ModuleList is itself also a module)\n",
        "    \"\"\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "\n",
        "\n",
        "# Problem 1\n",
        "\n",
        "class RNN(nn.Module):\n",
        "\n",
        "    \"\"\" A stacked vanilla RNN with Tanh nonlinearities.\"\"\"\n",
        "\n",
        "    def __init__(self, emb_size, hidden_size, seq_len, batch_size, vocab_size,\n",
        "\n",
        "                 num_layers, dp_keep_prob):\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        Initialization of the parameters of the recurrent and fc layers.\n",
        "\n",
        "        Supports any number of stacked hidden layers (specified by num_layers),\n",
        "\n",
        "        uses an input embedding layer, and includes fully connected layers with\n",
        "\n",
        "        dropout after each recurrent layer.\n",
        "\n",
        "\n",
        "\n",
        "        emb_size:     The number of units in the input embeddings\n",
        "\n",
        "        hidden_size:  The number of hidden units per layer\n",
        "\n",
        "        seq_len:      The length of the input sequences\n",
        "\n",
        "        vocab_size:   The number of tokens in the vocabulary (10,000 for Penn TreeBank)\n",
        "\n",
        "        num_layers:   The depth of the stack (i.e. the number of hidden layers at\n",
        "\n",
        "                      each time-step)\n",
        "\n",
        "        dp_keep_prob: The probability of *not* dropping out units in the\n",
        "\n",
        "                      non-recurrent connections.\n",
        "\n",
        "                      Dropout is not applied on the recurrent connections.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "\n",
        "        # Parameters\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.dp_keep_prob = dp_keep_prob\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Input Embedding Layer\n",
        "        self.embeddings = nn.Embedding(self.vocab_size,self.emb_size)\n",
        "                                       \n",
        "                                       \n",
        "\n",
        "\n",
        "\n",
        "        # Create layers\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        # The first layer\n",
        "\n",
        "        self.layers.append(nn.Linear(emb_size + hidden_size, hidden_size))\n",
        "\n",
        "        # The hidden layers\n",
        "\n",
        "        self.layers.extend(clones(nn.Linear(2*hidden_size, hidden_size), num_layers-1))\n",
        "\n",
        "        # Dropout\n",
        "\n",
        "        self.dropout = nn.Dropout(1 - self.dp_keep_prob)\n",
        "\n",
        "        # The output layer\n",
        "\n",
        "        self.out_layer = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "\n",
        "        # Initialize all weights\n",
        "        self.init_weights()\n",
        "\n",
        "\n",
        "\n",
        "    def init_weights(self):\n",
        "\n",
        "        \"\"\"Initialize the embedding and output weights uniformly.\"\"\"\n",
        "\n",
        "        # Intialize embedding weights unformly in the range [\n",
        "\n",
        "        nn.init.uniform_(self.embeddings.weight, -0.1, 0.1)\n",
        "\n",
        "        # For every layer\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "\n",
        "            # Initialize the weights and biases uniformly\n",
        "\n",
        "            b = 1/math.sqrt(self.hidden_size)\n",
        "\n",
        "            nn.init.uniform_(self.layers[i].weight, -b, b)\n",
        "\n",
        "            nn.init.uniform_(self.layers[i].bias, -b, b)\n",
        "\n",
        "        # Initialize output layer weights uniformly in the range [-0.1, 0.1]\n",
        "\n",
        "        # And all the biases to 0\n",
        "\n",
        "        nn.init.uniform_(self.out_layer.weight, -0.1, 0.1)\n",
        "\n",
        "        nn.init.zeros_(self.out_layer.bias)\n",
        "\n",
        "\n",
        "\n",
        "    def init_hidden(self):\n",
        "\n",
        "        \"\"\"Initialize the hidden states to zero.\n",
        "\n",
        "\n",
        "\n",
        "        This is used for the first mini-batch in an epoch, only.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        return torch.zeros(self.num_layers, self.batch_size, self.hidden_size)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, inputs, hidden):\n",
        "\n",
        "        \"\"\" Compute the recurrent updates.\n",
        "\n",
        "\n",
        "\n",
        "        Compute the forward pass, using nested python for loops.\n",
        "\n",
        "        The outer for loop iterates over timesteps, and the inner for loop iterates\n",
        "\n",
        "        over hidden layers of the stack.\n",
        "\n",
        "\n",
        "\n",
        "        Within these for loops, the parameter tensors and nn.modules\n",
        "\n",
        "        created in __init__ are used to compute the recurrent updates according to\n",
        "\n",
        "        the equations provided in the .tex of the assignment.\n",
        "\n",
        "\n",
        "\n",
        "        Arguments:\n",
        "\n",
        "            - inputs: A mini-batch of input sequences, composed of integers that\n",
        "\n",
        "                        represent the index of the current token(s) in the vocabulary.\n",
        "\n",
        "                            shape: (seq_len, batch_size)\n",
        "\n",
        "            - hidden: The initial hidden states for every layer of the stacked RNN.\n",
        "\n",
        "                            shape: (num_layers, batch_size, hidden_size)\n",
        "\n",
        "\n",
        "\n",
        "        Returns:\n",
        "\n",
        "            - Logits for the softmax over output tokens at every time-step.\n",
        "\n",
        "                  **Do NOT apply softmax to the outputs!**\n",
        "\n",
        "                  Pytorch's CrossEntropyLoss function (applied in run_exp.py) does\n",
        "\n",
        "                  this computation implicitly.\n",
        "\n",
        "                        shape: (seq_len, batch_size, vocab_size)\n",
        "\n",
        "            - The final hidden states for every layer of the stacked RNN.\n",
        "\n",
        "                  These will be used as the initial hidden states for all the\n",
        "\n",
        "                  mini-batches in an epoch, except for the first, where the return\n",
        "\n",
        "                  value of self.init_hidden will be used.\n",
        "\n",
        "                        shape: (num_layers, batch_size, hidden_size)\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        if inputs.is_cuda:\n",
        "\n",
        "            device = inputs.get_device()\n",
        "\n",
        "        else:\n",
        "\n",
        "            device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "        # Apply the Embedding layer on the input\n",
        "\n",
        "        embed_out = self.embeddings(inputs)# shape (seq_len,batch_size,emb_size)\n",
        "\n",
        "\n",
        "\n",
        "        # Create a tensor to store outputs during the Forward\n",
        "\n",
        "        logits = torch.zeros(self.seq_len, self.batch_size, self.vocab_size).to(device)\n",
        "\n",
        "\n",
        "\n",
        "        # For each time step\n",
        "\n",
        "        for timestep in range(self.seq_len):\n",
        "\n",
        "            # Apply dropout on the embedding result\n",
        "\n",
        "            input_ = self.dropout(embed_out[timestep])\n",
        "\n",
        "            # For each layer\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "\n",
        "                # Calculate the hidden states\n",
        "\n",
        "                # And apply the activation function tanh on it\n",
        "                \n",
        "                hidden[layer] = torch.tanh(self.layers[layer](torch.cat([input_, hidden[layer]], 1)))\n",
        "\n",
        "                # Apply dropout on this layer, but not for the recurrent units\n",
        "\n",
        "                input_ = self.dropout(hidden[layer])\n",
        "\n",
        "            # Store the output of the time step\n",
        "\n",
        "            logits[timestep] = self.out_layer(input_)\n",
        "\n",
        "\n",
        "\n",
        "        return logits, hidden\n",
        "\n",
        "\n",
        "\n",
        "    # Problem 4.2\n",
        "\n",
        "    def generate(self, inputs, hidden, generated_seq_len):\n",
        "        if inputs.is_cuda:\n",
        "            device = inputs.get_device()\n",
        "        else:\n",
        "            device = torch.device(\"cpu\")\n",
        "        logits = torch.zeros(generated_seq_len, self.batch_size).to(device)\n",
        "        \n",
        "        for timestep in range(generated_seq_len):\n",
        "            embed_out = self.embeddings(inputs)\n",
        "            input_ = embed_out\n",
        "            for layer in range(self.num_layers):\n",
        "                    hidden[layer] = torch.tanh(self.layers[layer](torch.cat([input_, hidden[layer]], 1)))  \n",
        "                    input_ = self.dropout(hidden[layer])\n",
        "            inputs = torch.argmax(self.out_layer(input_),dim=1)\n",
        "            logits[timestep] = torch.argmax(self.out_layer(input_),dim=1)        \n",
        "        return logits\n",
        "   \n",
        "\n",
        "# Problem 2\n",
        "\n",
        "class GRU(nn.Module): # Implement a stacked GRU RNN\n",
        "\n",
        "    \"\"\"A stacked gated recurrent unit (GRU) RNN.\n",
        "\n",
        "\n",
        "\n",
        "    Follow the same template as the RNN (above), but use the equations for\n",
        "\n",
        "    GRU, not Vanilla RNN.\n",
        "\n",
        "\n",
        "\n",
        "    Use the attribute names that are provided.\n",
        "\n",
        "\n",
        "\n",
        "    Initialize the embedding and output weights uniformly in the range [-0.1, 0.1]\n",
        "\n",
        "    and output biases to 0 (in place). The embeddings should not use a bias vector.\n",
        "\n",
        "    Initialize all other (i.e. recurrent and linear) weights AND biases uniformly\n",
        "\n",
        "    in the range [-k, k] where k is the square root of 1/hidden_size\n",
        "\n",
        "\n",
        "\n",
        "    IMPORTANT: For each init method, use a call to nn.init once for the weights\n",
        "\n",
        "    and once for the biases, in that order. If you follow the wrong order or\n",
        "\n",
        "    call nn.init a different number of times the Gradescope tests will fail.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, emb_size, hidden_size, seq_len, batch_size, vocab_size,\n",
        "\n",
        "               num_layers, dp_keep_prob):\n",
        "\n",
        "        super(GRU, self).__init__()\n",
        "\n",
        "        # Model parameters\n",
        "\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.dp_keep_prob = dp_keep_prob\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, emb_size)\n",
        "\n",
        "\n",
        "\n",
        "        # Create \"reset gate\" layers\n",
        "\n",
        "        self.r = nn.ModuleList()\n",
        "        self.r.append(nn.Linear(emb_size + hidden_size, hidden_size))\n",
        "        self.r.extend(clones(nn.Linear(2*hidden_size, hidden_size), num_layers-1))\n",
        "        \n",
        "\n",
        "        # \"forget gate\" layers\n",
        "\n",
        "        self.z = nn.ModuleList()\n",
        "        self.z.append(nn.Linear(emb_size + hidden_size, hidden_size))\n",
        "        self.z.extend(clones(nn.Linear(2*hidden_size, hidden_size), num_layers-1))\n",
        "        \n",
        "\n",
        "\n",
        "        # Create the \"memory content\" layers\n",
        "        self.h = nn.ModuleList()\n",
        "        self.h.append(nn.Linear(emb_size + hidden_size, hidden_size))\n",
        "        self.h.extend(clones(nn.Linear(2*hidden_size, hidden_size), num_layers-1))\n",
        "        \n",
        "        \n",
        "\n",
        "        # Dropout\n",
        "\n",
        "        self.dropout = nn.Dropout(p=(1 - dp_keep_prob))\n",
        "\n",
        "\n",
        "\n",
        "        # The output layer\n",
        "\n",
        "        # self.out_layer = nn.Linear(in_features=hidden_size,\n",
        "        #                            out_features=vocab_size,\n",
        "        #                            bias=True)\n",
        "\n",
        "        # The output layer\n",
        "        self.out_layer = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        self.init_embedding_weights_uniform()\n",
        "\n",
        "        self.init_reset_gate_weights_uniform()\n",
        "\n",
        "        self.init_forget_gate_weights_uniform()\n",
        "\n",
        "        self.init_memory_weights_uniform()\n",
        "\n",
        "        self.init_out_layer_weights_uniform()\n",
        "\n",
        "\n",
        "\n",
        "    def init_embedding_weights_uniform(self, init_range=0.1):\n",
        "        nn.init.uniform_(self.word_embeddings.weight, -0.1, 0.1)\n",
        "        \n",
        "    def init_reset_gate_weights_uniform(self):\n",
        "        \n",
        "        # TODO\n",
        "        for i in range(self.num_layers):\n",
        "            b = 1/math.sqrt(self.hidden_size)\n",
        "            nn.init.uniform_(self.r[i].weight, -b, b)\n",
        "            nn.init.uniform_(self.r[i].bias, -b, b)\n",
        "\n",
        "\n",
        "\n",
        "    def init_forget_gate_weights_uniform(self):\n",
        "        \n",
        "        # TODO = 0\n",
        "        for i in range(self.num_layers):\n",
        "            b = 1/math.sqrt(self.hidden_size)\n",
        "            nn.init.uniform_(self.z[i].weight, -b, b)\n",
        "            nn.init.uniform_(self.z[i].bias, -b, b)\n",
        "\n",
        "\n",
        "\n",
        "    def init_memory_weights_uniform(self):\n",
        "        \n",
        "        # TODO \n",
        "        for i in range(self.num_layers):\n",
        "            b = 1/math.sqrt(self.hidden_size)\n",
        "            nn.init.uniform_(self.h[i].weight, -b, b)\n",
        "            nn.init.uniform_(self.h[i].bias, -b, b)\n",
        "\n",
        "\n",
        "\n",
        "    def init_out_layer_weights_uniform(self):\n",
        "       nn.init.uniform_(self.out_layer.weight, -0.1, 0.1)\n",
        "       nn.init.zeros_(self.out_layer.bias)\n",
        "\n",
        "\n",
        "    def init_hidden(self):\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        This method returns a tensor of shape\n",
        "\n",
        "        (self.num_layers, self.batch_size, self.hidden_size)\n",
        "\n",
        "        filled with zeros as the initial hidden states of the GRU.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        return torch.zeros(self.num_layers, self.batch_size, self.hidden_size)\n",
        "\n",
        "    def forward(self, inputs, hidden):\n",
        "\n",
        "        \"\"\" Compute the recurrent updates.\n",
        "\n",
        "\n",
        "\n",
        "        Compute the forward pass, using nested python for loops.\n",
        "\n",
        "        The outer for loop iterates over timesteps, and the inner for loop iterates\n",
        "\n",
        "        over hidden layers of the stack.\n",
        "\n",
        "\n",
        "\n",
        "        Within these for loops, the parameter tensors and nn.modules\n",
        "\n",
        "        created in __init__ are used to compute the recurrent updates according to\n",
        "\n",
        "        the equations provided in the .tex of the assignment.\n",
        "\n",
        "\n",
        "\n",
        "        Arguments:\n",
        "\n",
        "            - inputs: A mini-batch of input sequences, composed of integers that\n",
        "\n",
        "                        represent the index of the current token(s) in the vocabulary.\n",
        "\n",
        "                            shape: (seq_len, batch_size)\n",
        "\n",
        "            - hidden: The initial hidden states for every layer of the stacked RNN.\n",
        "\n",
        "                            shape: (num_layers, batch_size, hidden_size)\n",
        "\n",
        "\n",
        "\n",
        "        Returns:\n",
        "\n",
        "            - Logits for the softmax over output tokens at every time-step.\n",
        "\n",
        "                  **Do NOT apply softmax to the outputs!**\n",
        "\n",
        "                  Pytorch's CrossEntropyLoss function (applied in run_exp.py) does\n",
        "\n",
        "                  this computation implicitly.\n",
        "\n",
        "                        shape: (seq_len, batch_size, vocab_size)\n",
        "\n",
        "            - The final hidden states for every layer of the stacked RNN.\n",
        "\n",
        "                  These will be used as the initial hidden states for all the\n",
        "\n",
        "                  mini-batches in an epoch, except for the first, where the return\n",
        "\n",
        "                  value of self.init_hidden will be used.\n",
        "\n",
        "                        shape: (num_layers, batch_size, hidden_size)\n",
        "\n",
        "        \"\"\"\n",
        "        if inputs.is_cuda:\n",
        "\n",
        "            device = inputs.get_device()\n",
        "\n",
        "        else:\n",
        "\n",
        "            device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "        # Apply the Embedding layer on the input\n",
        "\n",
        "        embed_out = self.word_embeddings(inputs)# shape (seq_len,batch_size,emb_size)\n",
        "\n",
        "\n",
        "        # Create a tensor to store outputs during the Forward\n",
        "\n",
        "        logits = torch.zeros(self.seq_len, self.batch_size, self.vocab_size).to(device)\n",
        "        \n",
        "        for timestep in range(self.seq_len):\n",
        "\n",
        "            # Apply dropout on the embedding result\n",
        "\n",
        "            input_ = self.dropout(embed_out[timestep])\n",
        "\n",
        "            # For each layer\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "        \n",
        "\n",
        "                # Calculate the hidden states\n",
        "\n",
        "                # And apply the activation function tanh on it\n",
        "                # r_result = torch.sigmoid(self.r[layer](torch.cat([input_, hidden[layer]], 1)))\n",
        "                # z_result = torch.sigmoid(self.z[layer](torch.cat([input_, hidden[layer]], 1)))\n",
        "                # h_result = torch.tanh(self.h[layer](torch.cat([input_, r_result*hidden[layer]], 1)))\n",
        "                # hidden[layer] = ((1-z_result)*hidden[layer]) + z_result*h_result\n",
        "           \n",
        "                r_result = torch.sigmoid(self.r[layer](torch.cat([input_, hidden[layer].clone()], 1)))\n",
        "                z_result = torch.sigmoid(self.z[layer](torch.cat([input_, hidden[layer].clone()], 1)))\n",
        "                \n",
        "                h_result = torch.tanh(self.h[layer] (torch.cat([input_, r_result * hidden[layer].clone()], 1)))\n",
        "                hidden[layer]= ((1 - z_result) * hidden[layer].clone()) + (z_result.clone()) * (h_result.clone())\n",
        "\n",
        "\n",
        "                # Apply dropout on this layer, but not for the recurrent unit\n",
        "                input_ = self.dropout(hidden[layer])\n",
        "\n",
        "            # Store the output of the time step\n",
        "            logits[timestep] = self.out_layer(input_)\n",
        "\n",
        "\n",
        "        return logits, hidden\n",
        "\n",
        "\n",
        "def generate(self, input, hidden, generated_seq_len):\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        Generate a sample sequence from the GRU.\n",
        "\n",
        "\n",
        "\n",
        "        This is similar to the forward method but instead of having ground\n",
        "\n",
        "        truth input for each time step, you are now required to sample the token\n",
        "\n",
        "        with maximum probability at each time step and feed it as input at the\n",
        "\n",
        "        next time step.\n",
        "\n",
        "\n",
        "\n",
        "        Arguments:\n",
        "\n",
        "            - input: A mini-batch of input tokens (NOT sequences!)\n",
        "\n",
        "                            shape: (batch_size)\n",
        "\n",
        "            - hidden: The initial hidden states for every layer of the stacked RNN.\n",
        "\n",
        "                            shape: (num_layers, batch_size, hidden_size)\n",
        "\n",
        "            - generated_seq_len: The length of the sequence to generate.\n",
        "\n",
        "                           Note that this can be different than the length used\n",
        "\n",
        "                           for training (self.seq_len)\n",
        "\n",
        "        Returns:\n",
        "\n",
        "            - Sampled sequences of tokens\n",
        "\n",
        "                        shape: (generated_seq_len, batch_size)\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO = 0\n",
        "        if input.is_cuda:\n",
        "\n",
        "            device = input.get_device()\n",
        "\n",
        "        else:\n",
        "\n",
        "            device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "        # Apply the Embedding layer on the input\n",
        "\n",
        "       # shape (seq_len,batch_size,emb_size)\n",
        "\n",
        "\n",
        "        # Create a tensor to store outputs during the Forward\n",
        "\n",
        "        logits = torch.zeros(generated_seq_len, self.batch_size)\n",
        "        \n",
        "        for timestep in range(generated_seq_len):\n",
        "\n",
        "            # Apply dropout on the embedding result\n",
        "            embed_out = self.word_embeddings(input)\n",
        "            input_ = embed_out\n",
        "\n",
        "            # For each layer\n",
        "\n",
        "            for layer in range(self.num_layers):\n",
        "        \n",
        "\n",
        "                # Calculate the hidden states\n",
        "\n",
        "                # And apply the activation function tanh on it\n",
        "                r_result = torch.sigmoid(self.r[layer](torch.cat([input_, hidden[layer]], 1)))\n",
        "                z_result = torch.sigmoid(self.z[layer](torch.cat([input_, hidden[layer]], 1)))\n",
        "                h_result = torch.tanh(self.h[layer](torch.cat([input_, r_result*hidden[layer]], 1)))\n",
        "                hidden[layer] = (1-z_result)*hidden[layer] + z_result*h_result\n",
        "                #hidden[layer] = torch.tanh(self.layers[layer](torch.cat([input_, hidden[layer]], 1)))\n",
        "\n",
        "                # Apply dropout on this layer, but not for the recurrent units\n",
        "\n",
        "                input_ = self.dropout(hidden[layer])\n",
        "\n",
        "            # Store the output of the time step\n",
        "            input = torch.argmax(self.out_layer(input_),dim=1)\n",
        "            logits[timestep] = torch.argmax(self.out_layer(input_),dim=1)\n",
        "        return logits\n",
        "        \n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "# Problem 2\n",
        "##############################################################################\n",
        "#\n",
        "# Code for the Transformer model\n",
        "#\n",
        "##############################################################################\n",
        "\n",
        "\"\"\"\n",
        "Implement the MultiHeadedAttention module of the transformer architecture.\n",
        "All other necessary modules have already been implemented for you.\n",
        "\n",
        "We're building a transfomer architecture for next-step prediction tasks, and\n",
        "applying it to sequential language modelling. We use a binary \"mask\" to specify\n",
        "which time-steps the model can use for the current prediction.\n",
        "This ensures that the model only attends to previous time-steps.\n",
        "\n",
        "The model first encodes inputs using the concatenation of a learned WordEmbedding\n",
        "and a (in our case, hard-coded) PositionalEncoding.\n",
        "The word embedding maps a word's one-hot encoding into a dense real vector.\n",
        "The positional encoding 'tags' each element of an input sequence with a code that\n",
        "identifies it's position (i.e. time-step).\n",
        "\n",
        "These encodings of the inputs are then transformed repeatedly using multiple\n",
        "copies of a TransformerBlock.\n",
        "This block consists of an application of MultiHeadedAttention, followed by a\n",
        "standard MLP; the MLP applies *the same* mapping at every position.\n",
        "Both the attention and the MLP are applied with Resnet-style skip connections,\n",
        "and layer normalization.\n",
        "\n",
        "The complete model consists of the embeddings, the stacked transformer blocks,\n",
        "and a linear layer followed by a softmax.\n",
        "\"\"\"\n",
        "\n",
        "#This code has been modified from an open-source project, by David Krueger.\n",
        "#The original license is included below:\n",
        "#MIT License\n",
        "#\n",
        "#Copyright (c) 2018 Alexander Rush\n",
        "#\n",
        "#Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "#of this software and associated documentation files (the \"Software\"), to deal\n",
        "#in the Software without restriction, including without limitation the rights\n",
        "#to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "#copies of the Software, and to permit persons to whom the Software is\n",
        "#furnished to do so, subject to the following conditions:\n",
        "#\n",
        "#The above copyright notice and this permission notice shall be included in all\n",
        "#copies or substantial portions of the Software.\n",
        "#\n",
        "#THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "#IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "#FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "#AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "#LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "#OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "#SOFTWARE.\n",
        "\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------------\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, n_heads, n_units, dropout=0.1):\n",
        "        \"\"\"\n",
        "        n_heads: the number of attention heads\n",
        "        n_units: the number of input and output units\n",
        "        dropout: probability of DROPPING units\n",
        "        \"\"\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        # This sets the size of the keys, values, and queries (self.d_k) to all\n",
        "        # be equal to the number of output units divided by the number of heads.\n",
        "        self.d_k = n_units // n_heads\n",
        "        # This requires the number of n_heads to evenly divide n_units.\n",
        "        assert n_units % n_heads == 0\n",
        "        self.n_units = n_units\n",
        "        self.n_heads = n_heads\n",
        "        # TODO ========================\n",
        "        # Create the layers below. self.linears should contain 3 linear\n",
        "        # layers that compute the projection from n_units => n_heads x d_k\n",
        "        # (one for each of query, key and value) plus an additional final layer\n",
        "        # (4 in total)\n",
        "\n",
        "        # Note: that parameters are initialized with Glorot initialization in\n",
        "        # the make_model function below (so you don't need to implement this\n",
        "        # yourself).\n",
        "\n",
        "        # Note: the only Pytorch modules you are allowed to use are nn.Linear\n",
        "        # and nn.Dropout. You can also use softmax, masked_fill and the \"clones\"\n",
        "        # function we provide.\n",
        "        self.linears = clones(nn.Linear(n_units, n_units), 4)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "\n",
        "    def attention(self, query, key, value, mask=None, dropout=None):\n",
        "        # Implement scaled dot product attention\n",
        "        # The query, key, and value inputs will be of size\n",
        "        # batch_size x n_heads x seq_len x d_k\n",
        "        # (If making a single call to attention in your forward method)\n",
        "        # and mask (if not None) will be of size\n",
        "        # batch_size x n_heads x seq_len x seq_len\n",
        "\n",
        "        # As described in the .tex, apply input masking to the softmax\n",
        "        # generating the \"attention values\" (i.e. A_i in the .tex)\n",
        "\n",
        "        # Also apply dropout to the attention values.\n",
        "        # This method needs to compare query and keys first, then mask positions\n",
        "        # if a mask is provided, normalize the scores, apply dropout and then\n",
        "        # retrieve values, in this particular order.\n",
        "        # When applying the mask, use values -1e9 for the masked positions.\n",
        "        # The method returns the result of the attention operation as well as\n",
        "        # the normalized scores after dropout.\n",
        "\n",
        "        # TODO ========================\n",
        "        d_k = query.size(-1)\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "             scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        norm_scores = F.softmax(scores, dim = -1)\n",
        "\n",
        "        if dropout is not None:\n",
        "           norm_scores =  dropout(norm_scores)   # Tensor of shape batch_size x n_heads x seq_len x seq_len\n",
        "        output = torch.matmul(norm_scores, value)# Tensor of shape batch_size x n_heads x seq_len x d_k\n",
        "\n",
        "        return output, norm_scores\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # Implement the masked multi-head attention.\n",
        "        # query, key, and value correspond to Q, K, and V in the latex, and\n",
        "        # they all have size: (batch_size, seq_len, self.n_units)\n",
        "        # mask has size: (batch_size, seq_len, seq_len)\n",
        "        # This method should call the attention method above\n",
        "        # TODO ========================\n",
        "        # 1) Do all the linear projections in batch from n_units => n_heads x d_k\n",
        "\n",
        "        # 2) Apply attention on all the projected vectors in batch.\n",
        "        # The query, key, value inputs to the attention method will be of size\n",
        "        # batch_size x n_heads x seq_len x d_k\n",
        "\n",
        "        # 3) \"Concat\" using a view and apply a final linear.\n",
        "\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        batch_size = query.size(0)\n",
        "        \n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
        "        query, key, value = [l(x).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "             for l, x in zip(self.linears, (query, key, value))]\n",
        "        \n",
        "        # 2) Apply attention on all the projected vectors in batch. \n",
        "        x, _ = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
        "        \n",
        "        # 3) \"Concat\" using a view and apply a final linear. \n",
        "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_k)\n",
        "        return self.linears[-1](x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------------\n",
        "# The encodings of elements of the input sequence\n",
        "\n",
        "class WordEmbedding(nn.Module):\n",
        "    def __init__(self, n_units, vocab):\n",
        "        super(WordEmbedding, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, n_units)\n",
        "        self.n_units = n_units\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print (x)\n",
        "        return self.lut(x) * math.sqrt(self.n_units)\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, n_units, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, n_units)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, n_units, 2).float() *\n",
        "                             -(math.log(10000.0) / n_units))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)],\n",
        "                         requires_grad=False)\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------------\n",
        "# The TransformerBlock and the full Transformer\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(ResidualSkipConnectionWithLayerNorm(size, dropout), 2)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) # apply the self-attention\n",
        "        return self.sublayer[1](x, self.feed_forward) # apply the position-wise MLP\n",
        "\n",
        "\n",
        "class TransformerStack(nn.Module):\n",
        "    \"\"\"\n",
        "    This will be called on the TransformerBlock (above) to create a stack.\n",
        "    \"\"\"\n",
        "    def __init__(self, layer, n_blocks): # layer will be TransformerBlock (below)\n",
        "        super(TransformerStack, self).__init__()\n",
        "        self.layers = clones(layer, n_blocks)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "class FullTransformer(nn.Module):\n",
        "    def __init__(self, transformer_stack, embedding, n_units, vocab_size):\n",
        "        super(FullTransformer, self).__init__()\n",
        "        self.transformer_stack = transformer_stack\n",
        "        self.embedding = embedding\n",
        "        self.output_layer = nn.Linear(n_units, vocab_size)\n",
        "\n",
        "    def forward(self, input_sequence, mask):\n",
        "        embeddings = self.embedding(input_sequence)\n",
        "        return F.log_softmax(self.output_layer(self.transformer_stack(embeddings, mask)), dim=-1)\n",
        "\n",
        "\n",
        "def make_model(vocab_size, n_blocks=6,\n",
        "               n_units=512, n_heads=16, dropout=0.1):\n",
        "    \"Helper: Construct a model from hyperparameters.\"\n",
        "    c = copy.deepcopy\n",
        "    attn = MultiHeadedAttention(n_heads, n_units)\n",
        "    ff = MLP(n_units, dropout)\n",
        "    position = PositionalEncoding(n_units, dropout)\n",
        "    model = FullTransformer(\n",
        "        transformer_stack=TransformerStack(TransformerBlock(n_units, c(attn), c(ff), dropout), n_blocks),\n",
        "        embedding=nn.Sequential(WordEmbedding(n_units, vocab_size), c(position)),\n",
        "        n_units=n_units,\n",
        "        vocab_size=vocab_size\n",
        "        )\n",
        "\n",
        "    # Initialize parameters with Glorot / fan_avg.\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "    return model\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------------\n",
        "# Data processing\n",
        "\n",
        "def subsequent_mask(size):\n",
        "    \"\"\" helper function for creating the masks. \"\"\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0\n",
        "\n",
        "class Batch:\n",
        "    \"Object for holding a batch of data with mask during training.\"\n",
        "    def __init__(self, x, pad=0):\n",
        "        self.data = x\n",
        "        self.mask = self.make_mask(self.data, pad)\n",
        "\n",
        "    @staticmethod\n",
        "    def make_mask(data, pad):\n",
        "        \"Create a mask to hide future words.\"\n",
        "        mask = (data != pad).unsqueeze(-2)\n",
        "        mask = mask & Variable(\n",
        "            subsequent_mask(data.size(-1)).type_as(mask.data))\n",
        "        return mask\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------------\n",
        "# Some standard modules\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"layer normalization, as in: https://arxiv.org/abs/1607.06450\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
        "\n",
        "\n",
        "class ResidualSkipConnectionWithLayerNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(ResidualSkipConnectionWithLayerNorm, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    This is just an MLP with 1 hidden layer\n",
        "    \"\"\"\n",
        "    def __init__(self, n_units, dropout=0.1):\n",
        "        super(MLP, self).__init__()\n",
        "        self.w_1 = nn.Linear(n_units, 2048)\n",
        "        self.w_2 = nn.Linear(2048, n_units)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKlxlw6OapHA"
      },
      "source": [
        "RUN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mr6s2dranxN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "2717b8f3-14fb-4dc6-ecb6-5e592a13773a"
      },
      "source": [
        "#!/bin/python\n",
        "# coding: utf-8\n",
        "\n",
        "# Code outline/scaffold for\n",
        "# ASSIGNMENT 2: RNNs, Attention, and Optimization\n",
        "# By Tegan Maharaj, David Krueger, and Chin-Wei Huang\n",
        "# Edits 2020 by Jessica Thompson, Jonathan Cornford and Lluis Castrejon\n",
        "# IFT6135 at University of Montreal\n",
        "# Winter 2020\n",
        "#\n",
        "# based on code from:\n",
        "#    https://github.com/deeplearningathome/pytorch-language-model/blob/master/reader.py\n",
        "#    https://github.com/ceshine/examples/blob/master/word_language_model/main.py\n",
        "#    https://github.com/teganmaharaj/zoneout/blob/master/zoneout_word_ptb.py\n",
        "#    https://github.com/harvardnlp/annotated-transformer\n",
        "\n",
        "## GENERAL INSTRUCTIONS\n",
        "\n",
        "# - We encourage you to read and understand this code; there are some notes and comments to help you.\n",
        "# - Typically, all of your code to submit should be written in solution.py;\n",
        "#  see further instructions at the top of that file / in TODOs.\n",
        "#      - GRU recurrent unit\n",
        "#      - Multi-head attention for the Transformer\n",
        "\n",
        "# - Other than this file and solution.py, you will probably also need to modify\n",
        "# and/or write additional code to create plots (learning curves, loss w.r.t.\n",
        "# time, gradients w.r.t. hiddens) and to load a saved model (computing gradients\n",
        "# w.r.t. hidden and for sampling from the model). This code will not be graded.\n",
        "\n",
        "\n",
        "# ## PROBLEM-SPECIFIC INSTRUCTIONS:\n",
        "#    - For Problem 3.1 the hyperparameter settings you should run are as follows\n",
        "#            --model=RNN --optimizer=SGD --initial_lr=1.0 --batch_size=128 --seq_len=35 --hidden_size=512 --num_layers=2 --dp_keep_prob=0.8  --num_epochs=20 --save_best\n",
        "#            --model=RNN --optimizer=SGD --initial_lr=1.0 --batch_size=20  --seq_len=35 --hidden_size=512 --num_layers=2 --dp_keep_prob=0.8  --num_epochs=20\n",
        "#            --model=RNN --optimizer=SGD --initial_lr=10.0 --batch_size=128 --seq_len=35 --hidden_size=512 --num_layers=2 --dp_keep_prob=0.8  --num_epochs=20\n",
        "#            --model=RNN --optimizer=ADAM --initial_lr=0.001 --batch_size=128 --seq_len=35 --hidden_size=512 --num_layers=2 --dp_keep_prob=0.8  --num_epochs=20\n",
        "#            --model=RNN --optimizer=ADAM --initial_lr=0.0001 --batch_size=128 --seq_len=35 --hidden_size=512 --num_layers=2 --dp_keep_prob=0.8  --num_epochs=20\n",
        "\n",
        "#    - For Problem 3.2 the hyperparameter settings you should run are as follows\n",
        "#            --model=GRU --optimizer=ADAM --initial_lr=0.001 --batch_size=128 --seq_len=35 --hidden_size=512 --num_layers=2 --dp_keep_prob=0.5  --num_epochs=20 --save_best\n",
        "#            --model=GRU --optimizer=SGD  --initial_lr=10.0  --batch_size=128 --seq_len=35 --hidden_size=512 --num_layers=2 --dp_keep_prob=0.5  --num_epochs=20\n",
        "#            --model=GRU --optimizer=ADAM --initial_lr=0.001 --batch_size=20 --seq_len=35 --hidden_size=512 --num_layers=2 --dp_keep_prob=0.5  --num_epochs=20\n",
        "\n",
        "#    - For Problem 3.3 the hyperparameter settings you should run are as follows\n",
        "#            --model=GRU --optimizer=ADAM --initial_lr=0.001 --batch_size=128 --seq_len=35 --hidden_size=256 --num_layers=2 --dp_keep_prob=0.2  --num_epochs=20\n",
        "#            --model=GRU --optimizer=ADAM --initial_lr=0.001 --batch_size=128 --seq_len=35 --hidden_size=2048 --num_layers=2 --dp_keep_prob=0.5  --num_epochs=20\n",
        "#            --model=GRU --optimizer=ADAM --initial_lr=0.001 --batch_size=128 --seq_len=35 --hidden_size=512 --num_layers=4 --dp_keep_prob=0.5  --num_epochs=20\n",
        "\n",
        "#    - For Problem 3.4 the hyperparameter settings you should run are as follows\n",
        "#            --model=TRANSFORMER --optimizer=ADAM --initial_lr=0.0001 --batch_size=128 --seq_len=35 --hidden_size=512  --num_layers=6 --dp_keep_prob=0.9 --num_epochs=20\n",
        "#            --model=TRANSFORMER --optimizer=ADAM --initial_lr=0.0001 --batch_size=128 --seq_len=35 --hidden_size=512  --num_layers=2 --dp_keep_prob=0.9 --num_epochs=20\n",
        "#            --model=TRANSFORMER --optimizer=ADAM --initial_lr=0.0001 --batch_size=128 --seq_len=35 --hidden_size=2048 --num_layers=2 --dp_keep_prob=0.6 --num_epochs=20\n",
        "#            --model=TRANSFORMER --optimizer=ADAM --initial_lr=0.0001 --batch_size=128 --seq_len=35 --hidden_size=1024 --num_layers=6 --dp_keep_prob=0.9 --num_epochs=20\n",
        "\n",
        "# You are also encouraged to explore the hyperparameter space and try to obtain\n",
        "# better validation perplexities than the given settings.\n",
        "#\n",
        "\n",
        "# - For Problem 4.1, perform all computations / plots based on saved models from\n",
        "#   Problem 3.1 and 3.2. Note the --save_best flag for the first set of\n",
        "#   parameters for each question (Of course you can still save other models than\n",
        "#   them if you like; just add the flag --save_best). You can modify the loss\n",
        "#   computation in this script (search for \"LOSS COMPUTATION\" to find the\n",
        "#   appropriate line.\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "import collections\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import numpy\n",
        "np = numpy\n",
        "\n",
        "# NOTE ==============================================\n",
        "# This is where your models are imported\n",
        "#from solution import RNN, GRU\n",
        "#from solution import make_model as TRANSFORMER\n",
        "\n",
        "##############################################################################\n",
        "#\n",
        "# ARG PARSING AND EXPERIMENT SETUP\n",
        "#\n",
        "##############################################################################\n",
        "\n",
        "parser = argparse.ArgumentParser(description='PyTorch Penn Treebank Language Modeling')\n",
        "\n",
        "# Arguments you may need to set to run different experiments in 4.1 & 4.2.\n",
        "parser.add_argument('--data', type=str, default='data',\n",
        "                    help='location of the data corpus. We suggest you change the default\\\n",
        "                    here, rather than passing as an argument, to avoid long file paths.')\n",
        "parser.add_argument('--model', type=str, default='RNN',\n",
        "                    help='type of recurrent net (RNN, GRU, TRANSFORMER)')\n",
        "parser.add_argument('--optimizer', type=str, default='SGD',\n",
        "                    help='optimization algo to use; SGD, SGD_LR_SCHEDULE, ADAM')\n",
        "parser.add_argument('--seq_len', type=int, default=35,\n",
        "                    help='number of timesteps over which BPTT is performed')\n",
        "parser.add_argument('--batch_size', type=int, default=128,\n",
        "                    help='size of one minibatch')\n",
        "parser.add_argument('--initial_lr', type=float, default=1.0,\n",
        "                    help='initial learning rate')\n",
        "parser.add_argument('--hidden_size', type=int, default=512,\n",
        "                    help='size of hidden layers. IMPORTANT: for the transformer\\\n",
        "                    this must be a multiple of 16.')\n",
        "parser.add_argument('--save_best', action='store_true',\n",
        "                    help='save the model for the best validation performance')\n",
        "parser.add_argument('--num_layers', type=int, default=2,\n",
        "                    help='number of hidden layers in RNN/GRU, or number of transformer blocks in TRANSFORMER')\n",
        "\n",
        "# Other hyperparameters you may want to tune in your exploration\n",
        "parser.add_argument('--emb_size', type=int, default=200,\n",
        "                    help='size of word embeddings')\n",
        "parser.add_argument('--num_epochs', type=int, default=1,\n",
        "                    help='number of epochs to stop after')\n",
        "parser.add_argument('--dp_keep_prob', type=float, default=0.8,\n",
        "                    help='dropout *keep* probability. drop_prob = 1-dp_keep_prob \\\n",
        "                    (dp_keep_prob=1 means no dropout)')\n",
        "\n",
        "# Arguments that you may want to make use of / implement more code for\n",
        "parser.add_argument('--debug', action='store_true')\n",
        "parser.add_argument('--save_dir', type=str, default='',\n",
        "                    help='path to save the experimental config, logs, model \\\n",
        "                    This is automatically generated based on the command line \\\n",
        "                    arguments you pass and only needs to be set if you want a \\\n",
        "                    custom dir name')\n",
        "parser.add_argument('--evaluate', action='store_true',\n",
        "                    help=\"use this flag to run on the test set. Only do this \\\n",
        "                    ONCE for each model setting, and only after you've \\\n",
        "                    completed ALL hyperparameter tuning on the validation set.\\\n",
        "                    Note we are not requiring you to do this.\")\n",
        "\n",
        "# DO NOT CHANGE THIS (setting the random seed makes experiments deterministic,\n",
        "# which helps for reproducibility)\n",
        "parser.add_argument('--seed', type=int, default=1111,\n",
        "                    help='random seed')\n",
        "\n",
        "def set_seed(seed):\n",
        "    # random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "args = parser.parse_args(args=[])\n",
        "argsdict = args.__dict__\n",
        "argsdict['code_file'] = sys.argv[0]\n",
        "\n",
        "# Use the model, optimizer, and the flags passed to the script to make the\n",
        "# name for the experimental dir\n",
        "print(\"\\n########## Setting Up Experiment ######################\")\n",
        "flags = [flag.lstrip('--').replace('/', '').replace('\\\\', '') for flag in sys.argv[1:]]\n",
        "experiment_path = os.path.join(args.save_dir, '_'.join([argsdict['model'],\n",
        "                                         argsdict['optimizer']]\n",
        "                                         + flags))\n",
        "# Increment a counter so that previous results with the same args will not\n",
        "# be overwritten. Comment out the next four lines if you only want to keep\n",
        "# the most recent results.\n",
        "i = 0\n",
        "while os.path.exists(experiment_path + \"_\" + str(i)):\n",
        "    i += 1\n",
        "experiment_path = experiment_path + \"_\" + str(i)\n",
        "\n",
        "# Creates an experimental directory and dumps all the args to a text file\n",
        "os.makedirs(experiment_path, exist_ok=True)\n",
        "\n",
        "print (\"\\nPutting log in %s\"%experiment_path)\n",
        "argsdict['save_dir'] = experiment_path\n",
        "with open (os.path.join(experiment_path,'exp_config.txt'), 'w') as f:\n",
        "    for key in sorted(argsdict):\n",
        "        f.write(key+'    '+str(argsdict[key])+'\\n')\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "set_seed(args.seed)\n",
        "\n",
        "\n",
        "# Use the GPU if you have one\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using the GPU\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"WARNING: You are about to run on cpu, and this will likely run out \\\n",
        "      of memory. \\n You can try setting batch_size=1 to reduce memory usage\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "#\n",
        "# LOADING & PROCESSING\n",
        "#\n",
        "###############################################################################\n",
        "\n",
        "# HELPER FUNCTIONS\n",
        "def _read_words(filename):\n",
        "    with open(filename, \"r\") as f:\n",
        "      return f.read().replace(\"\\n\", \"<eos>\").split()\n",
        "\n",
        "def _build_vocab(filename):\n",
        "    data = _read_words(filename)\n",
        "\n",
        "    counter = collections.Counter(data)\n",
        "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
        "\n",
        "    words, _ = list(zip(*count_pairs))\n",
        "    word_to_id = dict(zip(words, range(len(words))))\n",
        "    id_to_word = dict((v, k) for k, v in word_to_id.items())\n",
        "\n",
        "    with open(\"id_to_word.pickle\",\"wb\") as f:\n",
        "\n",
        "              pickle.dump(id_to_word,f)\n",
        "\n",
        "\n",
        "    return word_to_id, id_to_word\n",
        "\n",
        "def _file_to_word_ids(filename, word_to_id):\n",
        "    data = _read_words(filename)\n",
        "    return [word_to_id[word] for word in data if word in word_to_id]\n",
        "\n",
        "# Processes the raw data from text files\n",
        "def ptb_raw_data(data_path=None, prefix=\"ptb\"):\n",
        "    train_path = os.path.join(data_path, prefix + \".train.txt\")\n",
        "    valid_path = os.path.join(data_path, prefix + \".valid.txt\")\n",
        "    test_path = os.path.join(data_path, prefix + \".test.txt\")\n",
        "\n",
        "    word_to_id, id_2_word = _build_vocab(train_path)\n",
        "    train_data = _file_to_word_ids(train_path, word_to_id)\n",
        "    valid_data = _file_to_word_ids(valid_path, word_to_id)\n",
        "    test_data = _file_to_word_ids(test_path, word_to_id)\n",
        "    return train_data, valid_data, test_data, word_to_id, id_2_word\n",
        "\n",
        "# Yields minibatches of data\n",
        "def ptb_iterator(raw_data, batch_size, num_steps):\n",
        "    raw_data = np.array(raw_data, dtype=np.int32)\n",
        "\n",
        "    data_len = len(raw_data)\n",
        "    batch_len = data_len // batch_size\n",
        "    data = np.zeros([batch_size, batch_len], dtype=np.int32)\n",
        "    for i in range(batch_size):\n",
        "        data[i] = raw_data[batch_len * i:batch_len * (i + 1)]\n",
        "\n",
        "    epoch_size = (batch_len - 1) // num_steps\n",
        "\n",
        "    if epoch_size == 0:\n",
        "        raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
        "\n",
        "    for i in range(epoch_size):\n",
        "        x = data[:, i*num_steps:(i+1)*num_steps]\n",
        "        y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n",
        "        yield (x, y)\n",
        "\n",
        "\n",
        "class Batch:\n",
        "    \"Data processing for the transformer. This class adds a mask to the data.\"\n",
        "    def __init__(self, x, pad=-1):\n",
        "        self.data = x\n",
        "        self.mask = self.make_mask(self.data, pad)\n",
        "\n",
        "    @staticmethod\n",
        "    def make_mask(data, pad):\n",
        "        \"Create a mask to hide future words.\"\n",
        "\n",
        "        def subsequent_mask(size):\n",
        "            \"\"\" helper function for creating the masks. \"\"\"\n",
        "            attn_shape = (1, size, size)\n",
        "            subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "            return torch.from_numpy(subsequent_mask) == 0\n",
        "\n",
        "        mask = (data != pad).unsqueeze(-2)\n",
        "        mask = mask & Variable(\n",
        "            subsequent_mask(data.size(-1)).type_as(mask.data))\n",
        "        return mask\n",
        "\n",
        "\n",
        "# LOAD DATA\n",
        "print('Loading data from '+args.data)\n",
        "if args.data == 'SLURM_TMPDIR':\n",
        "    raw_data = ptb_raw_data(data_path=os.environ['SLURM_TMPDIR'])\n",
        "else:\n",
        "    raw_data = ptb_raw_data(data_path=args.data)\n",
        "train_data, valid_data, test_data, word_to_id, id_2_word = raw_data\n",
        "vocab_size = len(word_to_id)\n",
        "print('  vocabulary size: {}'.format(vocab_size))\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "#\n",
        "# MODEL SETUP\n",
        "#\n",
        "###############################################################################\n",
        "\n",
        "# NOTE ==============================================\n",
        "# This is where your model code will be called.\n",
        "if args.model == 'RNN':\n",
        "    model = RNN(emb_size=args.emb_size, hidden_size=args.hidden_size,\n",
        "                seq_len=args.seq_len, batch_size=args.batch_size,\n",
        "                vocab_size=vocab_size, num_layers=args.num_layers,\n",
        "                dp_keep_prob=args.dp_keep_prob)\n",
        "elif args.model == 'GRU':\n",
        "    model = GRU(emb_size=args.emb_size, hidden_size=args.hidden_size,\n",
        "                seq_len=args.seq_len, batch_size=args.batch_size,\n",
        "                vocab_size=vocab_size, num_layers=args.num_layers,\n",
        "                dp_keep_prob=args.dp_keep_prob)\n",
        "elif args.model == 'TRANSFORMER':\n",
        "    if args.debug:  # use a very small model\n",
        "        model = TRANSFORMER(vocab_size=vocab_size, n_units=16, n_blocks=2)\n",
        "    else:\n",
        "        # Note that we're using num_layers and hidden_size to mean slightly\n",
        "        # different things here than in the RNNs.\n",
        "        # Also, the Transformer also has other hyperparameters\n",
        "        # (such as the number of attention heads) which can change it's behavior.\n",
        "        model = TRANSFORMER(vocab_size=vocab_size, n_units=args.hidden_size,\n",
        "                            n_blocks=args.num_layers, dropout=1.-args.dp_keep_prob)\n",
        "    # these 3 attributes don't affect the Transformer's computations;\n",
        "    # they are only used in run_epoch\n",
        "    model.batch_size=args.batch_size\n",
        "    model.seq_len=args.seq_len\n",
        "    model.vocab_size=vocab_size\n",
        "else:\n",
        "    print(\"Model type not recognized.\")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# LOSS FUNCTION\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "if args.optimizer == 'ADAM':\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.initial_lr)\n",
        "\n",
        "# LEARNING RATE\n",
        "lr = args.initial_lr\n",
        "# These variables are for learning rate schedule (which you are not asked to use)\n",
        "# see SGD_LR_SCHEDULE in the main loop\n",
        "lr_decay_base = 1 / 1.15\n",
        "m_flat_lr = 14.0 # we will not touch lr for the first m_flat_lr epochs\n",
        "\n",
        "###############################################################################\n",
        "#\n",
        "# DEFINE COMPUTATIONS FOR PROCESSING ONE EPOCH\n",
        "#\n",
        "###############################################################################\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"\n",
        "    Wraps hidden states in new Tensors, to detach them from their history.\n",
        "\n",
        "    This prevents Pytorch from trying to backpropagate into previous input\n",
        "    sequences when we use the final hidden states from one mini-batch as the\n",
        "    initial hidden states for the next mini-batch.\n",
        "\n",
        "    Using the final hidden states in this way makes sense when the elements of\n",
        "    the mini-batches are actually successive subsequences in a set of longer sequences.\n",
        "    This is the case with the way we've processed the Penn Treebank dataset.\n",
        "    \"\"\"\n",
        "    if isinstance(h, Variable):\n",
        "        return h.detach_()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "\n",
        "def run_epoch(model, data, is_train=False, lr=1.0):\n",
        "    \"\"\"\n",
        "    One epoch of training/validation (depending on flag is_train).\n",
        "    \"\"\"\n",
        "    if is_train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    epoch_size = ((len(data) // model.batch_size) - 1) // model.seq_len\n",
        "    start_time = time.time()\n",
        "    if args.model != 'TRANSFORMER':\n",
        "        hidden = model.init_hidden()\n",
        "        hidden = hidden.to(device)\n",
        "    costs = 0.0\n",
        "    iters = 0\n",
        "    losses = []\n",
        "\n",
        "    # LOOP THROUGH MINIBATCHES\n",
        "    for step, (x, y) in enumerate(ptb_iterator(data, model.batch_size, model.seq_len)):\n",
        "        if args.model == 'TRANSFORMER':\n",
        "            batch = Batch(torch.from_numpy(x).long().to(device))\n",
        "            model.zero_grad()\n",
        "            outputs = model.forward(batch.data, batch.mask).transpose(1,0)\n",
        "            #print (\"outputs.shape\", outputs.shape)\n",
        "        else:\n",
        "            inputs = torch.from_numpy(x.astype(np.int64)).transpose(0, 1).contiguous().to(device)#.cuda()\n",
        "            model.zero_grad()\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            targets = torch.from_numpy(y.astype(np.int64)).transpose(0, 1).contiguous().to(device)#.cuda()\n",
        "            tt = torch.squeeze(targets.view(-1, model.batch_size * model.seq_len))\n",
        "            outputs, hidden = model(inputs, hidden)\n",
        "            \n",
        "            with open(\"myinput.pickle\",\"wb\") as f:\n",
        "              pickle.dump(inputs,f)\n",
        "\n",
        "\n",
        "            \n",
        "\n",
        "\n",
        "        # LOSS COMPUTATION\n",
        "        # This line currently averages across all the sequences in a mini-batch\n",
        "        # and all time-steps of the sequences.\n",
        "        # For problem 4.1, you will (instead) need to compute the average loss\n",
        "        # at each time-step separately. Hint: use the method retain_grad to keep\n",
        "        # gradients for intermediate nodes of the computational graph.\n",
        "        #\n",
        "        loss = loss_fn(outputs.contiguous().view(-1, model.vocab_size), tt)\n",
        "        costs += loss.data.item() * model.seq_len\n",
        "        losses.append(costs)\n",
        "        iters += model.seq_len\n",
        "        if args.debug:\n",
        "            print(step, loss)\n",
        "        if is_train:  # Only update parameters if training\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
        "            if args.optimizer == 'ADAM':\n",
        "                optimizer.step()\n",
        "            else:\n",
        "                for p in model.parameters():\n",
        "                    if p.grad is not None:\n",
        "                        p.data.add_(-lr, p.grad.data)\n",
        "            if step % (epoch_size // 10) == 10:\n",
        "                print('step: '+ str(step) + '\\t' \\\n",
        "                    + \"loss (sum over all examples' seen this epoch):\" + str(costs) + '\\t' \\\n",
        "                    + 'speed (wps):' + str(iters * model.batch_size / (time.time() - start_time)))\n",
        "    return np.exp(costs / iters), losses\n",
        "\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "#\n",
        "# RUN MAIN LOOP (TRAIN AND VAL)\n",
        "#\n",
        "###############################################################################\n",
        "\n",
        "print(\"\\n########## Running Main Loop ##########################\")\n",
        "train_ppls = []\n",
        "train_losses = []\n",
        "val_ppls = []\n",
        "val_losses = []\n",
        "best_val_so_far = np.inf\n",
        "times = []\n",
        "\n",
        "# In debug mode, only run one epoch\n",
        "if args.debug:\n",
        "    num_epochs = 1\n",
        "else:\n",
        "    num_epochs = args.num_epochs\n",
        "\n",
        "# MAIN LOOP\n",
        "for epoch in range(num_epochs):\n",
        "    t0 = time.time()\n",
        "    print('\\nEPOCH '+str(epoch)+' ------------------')\n",
        "    if args.optimizer == 'SGD_LR_SCHEDULE':\n",
        "        lr_decay = lr_decay_base ** max(epoch - m_flat_lr, 0)\n",
        "        lr = lr * lr_decay # decay lr if it is time\n",
        "\n",
        "    # RUN MODEL ON TRAINING DATA\n",
        "    train_ppl, train_loss = run_epoch(model, train_data, True, lr)\n",
        "\n",
        "    # RUN MODEL ON VALIDATION DATA\n",
        "    val_ppl, val_loss = run_epoch(model, valid_data)\n",
        "\n",
        "\n",
        "    # SAVE MODEL IF IT'S THE BEST SO FAR\n",
        "    if val_ppl < best_val_so_far:\n",
        "        best_val_so_far = val_ppl\n",
        "        if args.save_best:\n",
        "            print(\"Saving model parameters to best_params.pt\")\n",
        "            torch.save(model.state_dict(), os.path.join(args.save_dir, 'best_params.pt'))\n",
        "        # NOTE ==============================================\n",
        "        # You will need to load these parameters into the same model\n",
        "        # for a couple Problems: so that you can compute the gradient\n",
        "        # of the loss w.r.t. hidden state as required in Problem 4.1\n",
        "        # and to sample from the the model as required in Problem 4.2\n",
        "        # We are not asking you to run on the test data, but if you\n",
        "        # want to look at test performance you would load the saved\n",
        "        # model and run on the test data with batch_size=1\n",
        "\n",
        "    # LOC RESULTS\n",
        "    train_ppls.append(train_ppl)\n",
        "    val_ppls.append(val_ppl)\n",
        "    train_losses.extend(train_loss)\n",
        "    val_losses.extend(val_loss)\n",
        "    times.append(time.time() - t0)\n",
        "    log_str = 'epoch: ' + str(epoch) + '\\t' \\\n",
        "            + 'train ppl: ' + str(train_ppl) + '\\t' \\\n",
        "            + 'val ppl: ' + str(val_ppl)  + '\\t' \\\n",
        "            + 'best val: ' + str(best_val_so_far) + '\\t' \\\n",
        "            + 'time (s) spent in epoch: ' + str(times[-1])\n",
        "    print(log_str)\n",
        "    with open (os.path.join(args.save_dir, 'log.txt'), 'a') as f_:\n",
        "        f_.write(log_str+ '\\n')\n",
        "\n",
        "# SAVE LEARNING CURVES\n",
        "lc_path = os.path.join(args.save_dir, 'learning_curves.npy')\n",
        "print('\\nDONE\\n\\nSaving learning curves to '+lc_path)\n",
        "np.save(lc_path, {'train_ppls':train_ppls,\n",
        "                  'val_ppls':val_ppls,\n",
        "                  'train_losses':train_losses,\n",
        "                  'val_losses':val_losses,\n",
        "                  'times':times})\n",
        "# NOTE ==============================================\n",
        "# To load these, run\n",
        "# >>> x = np.load(lc_path, allow_pickle=True)[()]\n",
        "# You will need these values for plotting learning curves\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "########## Setting Up Experiment ######################\n",
            "\n",
            "Putting log in RNN_SGD_f_root.localsharejupyterruntimekernel-31825cd4-e972-4354-a099-094e5179a934.json_3\n",
            "Using the GPU\n",
            "Loading data from data\n",
            "  vocabulary size: 10000\n",
            "\n",
            "########## Running Main Loop ##########################\n",
            "\n",
            "EPOCH 0 ------------------\n",
            "step: 10\tloss (sum over all examples' seen this epoch):3201.6700077056885\tspeed (wps):21518.61148393303\n",
            "step: 30\tloss (sum over all examples' seen this epoch):8395.52184343338\tspeed (wps):22085.885556120156\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-a304707cfc80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;31m# RUN MODEL ON TRAINING DATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m     \u001b[0mtrain_ppl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;31m# RUN MODEL ON VALIDATION DATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-a304707cfc80>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(model, data, is_train, lr)\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only update parameters if training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ADAM'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFnOWQa8vKLN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "8e88d77d-cb33-408b-98a8-a6b3c3bf44e2"
      },
      "source": [
        "##########################\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, copy, time\n",
        "from torch.autograd import Variable\n",
        "import pickle\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using the GPU\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"WARNING: You are about to run on cpu, and this will likely run out \\\n",
        "      of memory. \\n You can try setting batch_size=1 to reduce memory usage\")\n",
        "    device = torch.device(\"cpu\")\n",
        "    \n",
        "with open('myinput.pickle', 'rb') as f:\n",
        "    loaded_obj = pickle.load(f)\n",
        "loaded_obj.shape\n",
        "loaded_obj_2 = loaded_obj[0:1,]\n",
        "print(loaded_obj_2.shape)\n",
        "hidden = model.init_hidden()\n",
        "hidden = hidden.to(device)\n",
        "hidden = repackage_hidden(hidden)\n",
        "# the model will be the best model from problem 3.1 and 3.2 \n",
        "inputs = loaded_obj_2.view(128).to(device)\n",
        "inputs = torch.tensor(inputs).to(device)\n",
        "\n",
        "## Select the number of timesteps\n",
        "samples = model.generate(inputs, hidden,35)\n",
        "samples = samples.transpose(0,1)\n",
        "\n",
        "\n",
        "with open('id_to_word.pickle', 'rb') as f:\n",
        "    id_to_word = pickle.load(f)\n",
        "\n",
        "\n",
        "row_num = 0\n",
        "for row in samples:\n",
        "  phrase = id_to_word[int(inputs[row_num])]\n",
        "  row_num = row_num +1\n",
        "  for column in range(len(row)):\n",
        "      phrase = phrase +' ' + id_to_word[int(row[column])] \n",
        "  print(phrase, '\\n')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using the GPU\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e9c8a6ebe974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'myinput.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mloaded_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mloaded_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'myinput.pickle'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-a3s1HdNiidS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}